import logging
import asyncio

# C·∫•u h√¨nh logging cho c·∫£ logging v√† print
logging.basicConfig(format="[RAG_Pipeline] %(message)s", level=logging.INFO)
logger = logging.getLogger(__name__)

# Ghi ƒë√® h√†m print ƒë·ªÉ th√™m prefix
original_print = print


def print(*args, **kwargs):
    prefix = "[RAG_Pipeline] "
    original_print(prefix + " ".join(map(str, args)), **kwargs)


# C·∫•u h√¨nh logging
logging.basicConfig(format="[RAG_Pipeline] %(message)s", level=logging.INFO)
logger = logging.getLogger(__name__)

from typing import List, Dict, AsyncGenerator
from backend.embedding import EmbeddingModel
from backend.llm import GeminiLLM
from backend.vector_store import VectorStore
from backend.document_processor import DocumentProcessor
from backend.prompt_manager import PromptManager
from backend.search import SearchManager
from backend.suggestion_manager import SuggestionManager
# from backend.query_processor import QueryProcessor
# from backend.query_router import QueryRouter
from backend.query_handler import QueryHandler
import os
import re
import concurrent.futures
from dotenv import load_dotenv
import time
import asyncio
import requests
import uuid
import json

# Import Google_Search
from backend.tools.Google_Search import run_query_with_sources as google_agent_search, get_raw_search_results

# Load bi·∫øn m√¥i tr∆∞·ªùng t·ª´ .env
load_dotenv()

# Khai b√°o c√°c bi·∫øn to√†n c·ª•c cho c√°c t√†i nguy√™n d√πng chung
global_embedding_model = None
global_llm_model = None
global_document_processor = None
global_prompt_manager = None
global_search_manager = None
global_resources_initialized = False


def initialize_global_resources():
    """Kh·ªüi t·∫°o c√°c t√†i nguy√™n d√πng chung to√†n c·ª•c m·ªôt l·∫ßn duy nh·∫•t"""
    global global_embedding_model, global_llm_model, global_document_processor, global_prompt_manager, global_search_manager, global_resources_initialized

    if not global_resources_initialized:
        print("B·∫Øt ƒë·∫ßu kh·ªüi t·∫°o c√°c t√†i nguy√™n to√†n c·ª•c...")

        # Kh·ªüi t·∫°o c√°c model ch·ªâ m·ªôt l·∫ßn
        global_embedding_model = EmbeddingModel()
        print("ƒê√£ kh·ªüi t·∫°o embedding model to√†n c·ª•c")

        global_llm_model = GeminiLLM()
        print("ƒê√£ kh·ªüi t·∫°o LLM to√†n c·ª•c")

        global_document_processor = DocumentProcessor()
        print("ƒê√£ kh·ªüi t·∫°o Document Processor to√†n c·ª•c")

        global_prompt_manager = PromptManager()
        print("ƒê√£ kh·ªüi t·∫°o Prompt Manager to√†n c·ª•c")

        # Search manager s·∫Ω ƒë∆∞·ª£c t·∫°o ri√™ng cho t·ª´ng user ƒë·ªÉ tr√°nh conflict
        # Kh√¥ng t·∫°o global search manager ƒë·ªÉ tr√°nh v·∫•n ƒë·ªÅ v·ªõi BM25 index
        global_search_manager = None
        print("Search Manager s·∫Ω ƒë∆∞·ª£c t·∫°o ri√™ng cho t·ª´ng user khi c·∫ßn thi·∫øt")

        global_resources_initialized = True
        print("Ho√†n th√†nh kh·ªüi t·∫°o t·∫•t c·∫£ t√†i nguy√™n to√†n c·ª•c")
    else:
        print("C√°c t√†i nguy√™n to√†n c·ª•c ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o tr∆∞·ªõc ƒë√≥")

    return {
        "embedding_model": global_embedding_model,
        "llm_model": global_llm_model,
        "document_processor": global_document_processor,
        "prompt_manager": global_prompt_manager,
        "search_manager": global_search_manager,
    }


class AdvancedDatabaseRAG:
    """L·ªõp ch√≠nh k·∫øt h·ª£p t·∫•t c·∫£ c√°c th√†nh ph·∫ßn c·ªßa h·ªá th·ªëng RAG"""

    def __init__(
        self,
        api_key=None,
        user_id=None,
        embedding_model=None,
        llm_model=None,
        document_processor=None,
        prompt_manager=None,
        search_manager=None,
    ):
        """Kh·ªüi t·∫°o h·ªá th·ªëng RAG"""
        # Kh·ªüi t·∫°o c√°c th√†nh ph·∫ßn ri√™ng bi·ªát t·ª´ b√™n ngo√†i ho·∫∑c t·∫°o m·ªõi
        if embedding_model is not None:
            self.embedding_model = embedding_model
            print("S·ª≠ d·ª•ng embedding model ƒë∆∞·ª£c cung c·∫•p t·ª´ b√™n ngo√†i")
        else:
            print("Kh·ªüi t·∫°o embedding model m·ªõi")
            self.embedding_model = EmbeddingModel()

        if llm_model is not None:
            self.llm = llm_model
            print("S·ª≠ d·ª•ng LLM ƒë∆∞·ª£c cung c·∫•p t·ª´ b√™n ngo√†i")
        else:
            print("Kh·ªüi t·∫°o LLM m·ªõi")
            self.llm = GeminiLLM(api_key)

        # L∆∞u tr·ªØ user_id
        self.user_id = user_id
        # Kh·ªüi t·∫°o vector store v·ªõi user_id
        self.vector_store = VectorStore(user_id=user_id)
        print(f"Kh·ªüi t·∫°o h·ªá th·ªëng RAG cho user_id={user_id}")

        if document_processor is not None:
            self.document_processor = document_processor
            print("S·ª≠ d·ª•ng Document Processor ƒë∆∞·ª£c cung c·∫•p t·ª´ b√™n ngo√†i")
        else:
            print("Kh·ªüi t·∫°o Document Processor m·ªõi")
            self.document_processor = DocumentProcessor()

        if prompt_manager is not None:
            self.prompt_manager = prompt_manager
            print("S·ª≠ d·ª•ng Prompt Manager ƒë∆∞·ª£c cung c·∫•p t·ª´ b√™n ngo√†i")
        else:
            print("Kh·ªüi t·∫°o Prompt Manager m·ªõi")
            self.prompt_manager = PromptManager()

        # S·ª≠ d·ª•ng search_manager t·ª´ b√™n ngo√†i ho·∫∑c t·∫°o m·ªõi v·ªõi vector_store c·ªßa user
        if search_manager is not None:
            # G√°n search_manager to√†n c·ª•c v√† c·∫≠p nh·∫≠t vector_store (c√πng v·ªõi BM25 index t∆∞∆°ng ·ª©ng)
            self.search_manager = search_manager
            # # Thay v√¨ ch·ªâ g√°n vector_store, g·ªçi ph∆∞∆°ng th·ª©c ƒë·ªÉ c·∫≠p nh·∫≠t v√† t·∫£i l·∫°i BM25 index ph√π h·ª£p
            # self.search_manager.set_vector_store_and_reload_bm25(self.vector_store)
            # print("S·ª≠ d·ª•ng Search Manager to√†n c·ª•c (ƒë√£ c·∫≠p nh·∫≠t vector_store v√† BM25 index cho user)")
        else:
            print("Kh·ªüi t·∫°o Search Manager m·ªõi")
            self.search_manager = SearchManager(self.vector_store, self.embedding_model)

        # # Th√™m QueryProcessor cho vi·ªác x·ª≠ l√Ω ƒë·ªìng tham chi·∫øu
        # self.query_processor = QueryProcessor()
        # print("ƒê√£ kh·ªüi t·∫°o QueryProcessor v·ªõi kh·∫£ nƒÉng x·ª≠ l√Ω ƒë·ªìng tham chi·∫øu")

        # # Th√™m QueryRouter cho vi·ªác ph√¢n lo·∫°i c√¢u h·ªèi
        # self.query_router = QueryRouter()
        # print("ƒê√£ kh·ªüi t·∫°o QueryRouter ƒë·ªÉ ph√¢n lo·∫°i c√¢u h·ªèi th√†nh 3 lo·∫°i")

                # H·ª£p nh·∫•t QueryProcessor v√† QueryRouter th√†nh QueryHandler
        self.query_handler = QueryHandler()
        print("ƒê√£ kh·ªüi t·∫°o QueryHandler ƒë·ªÉ x·ª≠ l√Ω v√† ph√¢n lo·∫°i c√¢u h·ªèi")

        # Kh·ªüi t·∫°o SuggestionManager cho vi·ªác t·∫°o c√¢u h·ªèi li√™n quan
        self.suggestion_manager = SuggestionManager(llm=self.llm)
        print("ƒê√£ kh·ªüi t·∫°o SuggestionManager ƒë·ªÉ t·∫°o c√¢u h·ªèi g·ª£i √Ω")

        # T√≠nh to√°n v√† l∆∞u c√°c gi√° tr·ªã kh√°c n·∫øu c·∫ßn
        self.enable_fact_checking = (
            False  # T√≠nh nƒÉng ki·ªÉm tra s·ª± ki·ªán (c√≥ th·ªÉ k√≠ch ho·∫°t sau)
        )

        self.confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", "0.5"))

        # C√†i ƒë·∫∑t c∆° ch·∫ø x·ª≠ l√Ω song song
        self.max_workers = int(os.getenv("MAX_PARALLEL_WORKERS", "4"))

    async def load_documents_async(self, data_dir: str) -> List[Dict]:
        """T·∫£i t√†i li·ªáu t·ª´ th∆∞ m·ª•c (b·∫•t ƒë·ªìng b·ªô)"""
        return await self.document_processor.load_documents(data_dir)

    def load_documents(self, data_dir: str) -> List[Dict]:
        """T·∫£i t√†i li·ªáu t·ª´ th∆∞ m·ª•c (ƒë·ªìng b·ªô)"""
        return self.document_processor.load_documents_sync(data_dir)

    async def process_documents_async(self, documents: List[Dict]) -> List[Dict]:
        """X·ª≠ l√Ω v√† chia nh·ªè t√†i li·ªáu v·ªõi x·ª≠ l√Ω song song (b·∫•t ƒë·ªìng b·ªô)"""
        # X·ª≠ l√Ω song song n·∫øu c√≥ nhi·ªÅu t√†i li·ªáu
        if len(documents) > 5:  # Ch·ªâ x·ª≠ l√Ω song song khi c√≥ nhi·ªÅu t√†i li·ªáu
            print(
                f"X·ª≠ l√Ω song song {len(documents)} t√†i li·ªáu v·ªõi {self.max_workers} workers"
            )
            chunks = []

            # S·ª≠ d·ª•ng asyncio ƒë·ªÉ x·ª≠ l√Ω song song
            tasks = []
            semaphore = asyncio.Semaphore(self.max_workers)
            
            async def process_with_semaphore(doc):
                async with semaphore:
                    return await self._process_single_document_async(doc)
            
            for doc in documents:
                task = asyncio.create_task(process_with_semaphore(doc))
                tasks.append(task)
            
            # Ch·ªù t·∫•t c·∫£ tasks ho√†n th√†nh
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    print(f"L·ªói khi x·ª≠ l√Ω t√†i li·ªáu {i}: {str(result)}")
                else:
                    chunks.extend(result)

            print(f"ƒê√£ x·ª≠ l√Ω xong {len(documents)} t√†i li·ªáu, t·∫°o ra {len(chunks)} chunks")
            return chunks

        else:
            # X·ª≠ l√Ω tu·∫ßn t·ª± cho √≠t t√†i li·ªáu
            print(f"X·ª≠ l√Ω tu·∫ßn t·ª± {len(documents)} t√†i li·ªáu")
            all_chunks = []
            for doc in documents:
                chunks = await self._process_single_document_async(doc)
                all_chunks.extend(chunks)

            print(f"ƒê√£ x·ª≠ l√Ω xong {len(documents)} t√†i li·ªáu, t·∫°o ra {len(all_chunks)} chunks")
            return all_chunks

    def process_documents(self, documents: List[Dict]) -> List[Dict]:
        """X·ª≠ l√Ω v√† chia nh·ªè t√†i li·ªáu v·ªõi x·ª≠ l√Ω song song (ƒë·ªìng b·ªô)"""
        # X·ª≠ l√Ω song song n·∫øu c√≥ nhi·ªÅu t√†i li·ªáu
        if len(documents) > 5:  # Ch·ªâ x·ª≠ l√Ω song song khi c√≥ nhi·ªÅu t√†i li·ªáu
            print(
                f"X·ª≠ l√Ω song song {len(documents)} t√†i li·ªáu v·ªõi {self.max_workers} workers"
            )
            chunks = []

            # S·ª≠ d·ª•ng ThreadPoolExecutor ƒë·ªÉ x·ª≠ l√Ω song song
            with concurrent.futures.ThreadPoolExecutor(
                max_workers=self.max_workers
            ) as executor:
                # Submit t√°c v·ª• x·ª≠ l√Ω cho t·ª´ng t√†i li·ªáu
                future_to_doc = {
                    executor.submit(self._process_single_document, doc): i
                    for i, doc in enumerate(documents)
                }

                # Thu th·∫≠p k·∫øt qu·∫£ khi ho√†n th√†nh
                for future in concurrent.futures.as_completed(future_to_doc):
                    doc_index = future_to_doc[future]
                    try:
                        doc_chunks = future.result()
                        chunks.extend(doc_chunks)
                        print(f"ƒê√£ x·ª≠ l√Ω xong t√†i li·ªáu {doc_index}")
                    except Exception as exc:
                        print(f"L·ªói khi x·ª≠ l√Ω t√†i li·ªáu {doc_index}: {exc}")

            print(f"ƒê√£ x·ª≠ l√Ω xong {len(documents)} t√†i li·ªáu, t·∫°o ra {len(chunks)} chunks")
            return chunks

        else:
            # X·ª≠ l√Ω tu·∫ßn t·ª± cho √≠t t√†i li·ªáu
            print(f"X·ª≠ l√Ω tu·∫ßn t·ª± {len(documents)} t√†i li·ªáu")
            all_chunks = []
            for doc in documents:
                chunks = self._process_single_document(doc)
                all_chunks.extend(chunks)

            print(f"ƒê√£ x·ª≠ l√Ω xong {len(documents)} t√†i li·ªáu, t·∫°o ra {len(all_chunks)} chunks")
            return all_chunks

    async def _process_single_document_async(self, document: Dict) -> List[Dict]:
        """X·ª≠ l√Ω m·ªôt t√†i li·ªáu ƒë∆°n l·∫ª (b·∫•t ƒë·ªìng b·ªô)"""
        return await asyncio.get_event_loop().run_in_executor(
            None, self._process_single_document, document
        )

    def _process_single_document(self, document: Dict) -> List[Dict]:
        """X·ª≠ l√Ω m·ªôt t√†i li·ªáu ƒë∆°n l·∫ª"""
        return self.document_processor.chunk_documents_sync([document])

    async def index_to_qdrant_async(self, chunks: List[Dict], user_id) -> None:
        """Index chunks l√™n Qdrant (b·∫•t ƒë·ªìng b·ªô)"""
        if not chunks:
            print("Kh√¥ng c√≥ chunks n√†o ƒë·ªÉ index")
            return

        start_time = time.time()
        print(f"B·∫Øt ƒë·∫ßu index {len(chunks)} chunks l√™n Qdrant cho user_id={user_id}")

        try:
            # T·∫°o embeddings batch ƒë·ªÉ t·ªëi ∆∞u t·ªëc ƒë·ªô
            texts = [chunk["text"] for chunk in chunks]
            print(f"ƒêang t·∫°o embeddings cho {len(texts)} texts...")

            # S·ª≠ d·ª•ng async batch encoding
            embeddings = await self.embedding_model.encode_batch(texts)
            print(f"ƒê√£ t·∫°o xong {len(embeddings)} embeddings")

            # L·∫•y file_id t·ª´ chunk ƒë·∫ßu ti√™n (gi·∫£ s·ª≠ t·∫•t c·∫£ chunks thu·ªôc c√πng m·ªôt file)
            file_id = chunks[0].get("file_id", str(uuid.uuid4()))

            # Index l√™n vector store
            await self.vector_store.index_documents(chunks, embeddings, user_id, file_id)

            end_time = time.time()
            processing_time = end_time - start_time
            print(
                f"Ho√†n th√†nh index {len(chunks)} chunks trong {processing_time:.2f}s "
                f"(trung b√¨nh {processing_time/len(chunks):.3f}s/chunk)"
            )

        except Exception as e:
            print(f"L·ªói khi index chunks: {str(e)}")
            raise

    def index_to_qdrant(self, chunks: List[Dict], user_id) -> None:
        """Index chunks l√™n Qdrant (ƒë·ªìng b·ªô)"""
        if not chunks:
            print("Kh√¥ng c√≥ chunks n√†o ƒë·ªÉ index")
            return

        start_time = time.time()
        print(f"B·∫Øt ƒë·∫ßu index {len(chunks)} chunks l√™n Qdrant cho user_id={user_id}")

        try:
            # T·∫°o embeddings batch ƒë·ªÉ t·ªëi ∆∞u t·ªëc ƒë·ªô
            texts = [chunk["text"] for chunk in chunks]
            print(f"ƒêang t·∫°o embeddings cho {len(texts)} texts...")

            # S·ª≠ d·ª•ng sync batch encoding
            embeddings = self.embedding_model.encode_batch_sync(texts)
            print(f"ƒê√£ t·∫°o xong {len(embeddings)} embeddings")

            # L·∫•y file_id t·ª´ chunk ƒë·∫ßu ti√™n (gi·∫£ s·ª≠ t·∫•t c·∫£ chunks thu·ªôc c√πng m·ªôt file)
            file_id = chunks[0].get("file_id", str(uuid.uuid4()))

            # Index l√™n vector store
            self.vector_store.index_documents_sync(chunks, embeddings, user_id, file_id)

            end_time = time.time()
            processing_time = end_time - start_time
            print(
                f"Ho√†n th√†nh index {len(chunks)} chunks trong {processing_time:.2f}s "
                f"(trung b√¨nh {processing_time/len(chunks):.3f}s/chunk)"
            )

        except Exception as e:
            print(f"L·ªói khi index chunks: {str(e)}")
            raise

    async def semantic_search_async(
        self,
        query: str,
        k: int = 20,
        sources: List[str] = None,
        file_id: List[str] = None,
    ) -> List[Dict]:
        """T√¨m ki·∫øm ng·ªØ nghƒ©a (b·∫•t ƒë·ªìng b·ªô)"""
        print(f"Semantic search v·ªõi query='{query}', k={k}")
        
        if sources:
            print(f"Filtering by sources: {sources}")
        if file_id:
            print(f"Filtering by file_id: {file_id}")

        try:
            # S·ª≠ d·ª•ng search manager v·ªõi async
            results = await self.search_manager.semantic_search(
                query=query,
                k=k,
                sources=sources,
                file_id=file_id,
            )

            print(f"T√¨m th·∫•y {len(results)} k·∫øt qu·∫£ t·ª´ semantic search")

            # Log th√¥ng tin k·∫øt qu·∫£ ƒë·ªÉ debug
            if results:
                for i, result in enumerate(results[:3]):  # Log 3 k·∫øt qu·∫£ ƒë·∫ßu
                    score = result.get("score", 0)
                    source = result.get("source", "unknown")
                    text_preview = result.get("text", "")[:100] + "..." if result.get("text", "") else ""
                    print(f"  Result {i+1}: score={score:.4f}, source={source}, text={text_preview}")

            return results

        except Exception as e:
            print(f"L·ªói trong semantic search: {str(e)}")
            import traceback
            print(f"Chi ti·∫øt l·ªói: {traceback.format_exc()}")
            return []

    def semantic_search(
        self,
        query: str,
        k: int = 20,
        sources: List[str] = None,
        file_id: List[str] = None,
    ) -> List[Dict]:
        """T√¨m ki·∫øm ng·ªØ nghƒ©a (ƒë·ªìng b·ªô)"""
        print(f"Semantic search v·ªõi query='{query}', k={k}")
        
        if sources:
            print(f"Filtering by sources: {sources}")
        if file_id:
            print(f"Filtering by file_id: {file_id}")

        try:
            # S·ª≠ d·ª•ng search manager v·ªõi sync
            results = self.search_manager.semantic_search_sync(
                query=query,
                k=k,
                sources=sources,
                file_id=file_id,
            )

            print(f"T√¨m th·∫•y {len(results)} k·∫øt qu·∫£ t·ª´ semantic search")

            # Log th√¥ng tin k·∫øt qu·∫£ ƒë·ªÉ debug
            if results:
                for i, result in enumerate(results[:3]):  # Log 3 k·∫øt qu·∫£ ƒë·∫ßu
                    score = result.get("score", 0)
                    source = result.get("source", "unknown")
                    text_preview = result.get("text", "")[:100] + "..." if result.get("text", "") else ""
                    print(f"  Result {i+1}: score={score:.4f}, source={source}, text={text_preview}")

            return results

        except Exception as e:
            print(f"L·ªói trong semantic search: {str(e)}")
            import traceback
            print(f"Chi ti·∫øt l·ªói: {traceback.format_exc()}")
            return []

    async def rerank_results_async(self, query: str, results: List[Dict]) -> List[Dict]:
        """T√°i x·∫øp h·∫°ng k·∫øt qu·∫£ (b·∫•t ƒë·ªìng b·ªô)"""
        return await self.search_manager.rerank_results(query, results)

    def rerank_results(self, query: str, results: List[Dict]) -> List[Dict]:
        """T√°i x·∫øp h·∫°ng k·∫øt qu·∫£ (ƒë·ªìng b·ªô)"""
        return self.search_manager.rerank_results_sync(query, results)

    async def query_with_sources_streaming(
        self,
        query: str,
        k: int = 20,
        sources: List[str] = None,
        file_id: List[str] = None,
        conversation_history: str = None,
    ) -> AsyncGenerator[Dict, None]:
        """
        Truy v·∫•n h·ªá th·ªëng RAG v·ªõi c√°c ngu·ªìn v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng stream

        Args:
            query: C√¢u h·ªèi ng∆∞·ªùi d√πng
            k: S·ªë l∆∞·ª£ng k·∫øt qu·∫£ tr·∫£ v·ªÅ
            sources: Danh s√°ch c√°c file ngu·ªìn c·∫ßn t√¨m ki·∫øm (c√°ch c≈©, s·ª≠ d·ª•ng file_id thay th·∫ø)
            file_id: Danh s√°ch c√°c file_id c·∫ßn t√¨m ki·∫øm (c√°ch m·ªõi). N·∫øu l√† None ho·∫∑c r·ªóng, s·∫Ω t√¨m ki·∫øm trong t·∫•t c·∫£ c√°c file
            conversation_history: L·ªãch s·ª≠ h·ªôi tho·∫°i

        Returns:
            AsyncGenerator tr·∫£ v·ªÅ t·ª´ng ph·∫ßn c·ªßa c√¢u tr·∫£ l·ªùi
        """
        print(f"ƒêang x·ª≠ l√Ω c√¢u h·ªèi (stream): '{query}'")
        
        if file_id is None or len(file_id) == 0:
            print("file_id l√† None ho·∫∑c danh s√°ch r·ªóng. T√¨m ki·∫øm s·∫Ω ƒë∆∞·ª£c th·ª±c hi·ªán tr√™n to√†n b·ªô t√†i li·ªáu.")
            # ƒê·∫∑t file_id th√†nh None ƒë·ªÉ semantic_search hi·ªÉu l√† t√¨m ki·∫øm tr√™n t·∫•t c·∫£ t√†i li·ªáu
            file_id = None
        else:
            print(f"T√¨m ki·∫øm v·ªõi file_id: {file_id}")

        # B·∫Øt ƒë·∫ßu ƒëo th·ªùi gian x·ª≠ l√Ω
        start_time = time.time()

        # X·ª≠ l√Ω v√† ph√¢n lo·∫°i c√¢u h·ªèi trong m·ªôt l·ªánh g·ªçi LLM duy nh·∫•t
        original_query = query
        expanded_query, query_type = await self.query_handler.expand_and_classify_query(
            query, conversation_history
        )
        query_to_use = expanded_query
        print(f"C√¢u h·ªèi ƒë√£ x·ª≠ l√Ω: '{query_to_use}', Lo·∫°i: '{query_type}'")

        # Tr·∫£ v·ªÅ ngay n·∫øu l√† c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn c∆° s·ªü d·ªØ li·ªáu
        if query_type == "other_question":
            # Tr·∫£ v·ªÅ th√¥ng b√°o b·∫Øt ƒë·∫ßu
            yield {
                "type": "start",
                "data": {
                    "query_type": query_type,
                    "file_id": file_id
                },
            }

            # Tr·∫£ v·ªÅ ngu·ªìn r·ªóng
            yield {
                "type": "sources",
                "data": {
                    "sources": [],
                    "filtered_sources": [],
                    "filtered_file_id": file_id if file_id else [],
                },
            }

            # Tr·∫£ v·ªÅ n·ªôi dung - ƒë√£ ƒë·ªãnh d·∫°ng Markdown
            response = await self.query_handler.get_response_for_other_question(query)
            yield {"type": "content", "data": {"content": response}}

            # Tr·∫£ v·ªÅ k·∫øt th√∫c
            elapsed_time = time.time() - start_time
            yield {
                "type": "end",
                "data": {
                    "processing_time": round(elapsed_time, 2),
                    "query_type": query_type,
                },
            }
            return
        
        # X·ª≠ l√Ω sql_code_task tr·ª±c ti·∫øp v·ªõi LLM (streaming)
        elif query_type == "sql_code_task":
            print(f"C√¢u h·ªèi ƒë∆∞·ª£c ph√¢n lo·∫°i l√† sql_code_task (stream): '{query_to_use}'")
            yield {
                "type": "start",
                "data": {
                    "query": original_query,
                    "expanded_query": query_to_use if query_to_use != original_query else None,
                    "query_type": "sql_code_task",
                    "file_id": file_id,
                },
            }

            yield {
                "type": "sources",
                "data": {"sources": [], "filtered_file_id": file_id if file_id else []},
            }
            
            prompt_template = self.prompt_manager.templates.get("sql_code_task_prompt")
            if not prompt_template:
                yield {"type": "content", "data": {"content": "L·ªói: Kh√¥ng t√¨m th·∫•y prompt template cho SQL code task."}}
            else:
                # Chu·∫©n b·ªã ng·ªØ c·∫£nh h·ªôi tho·∫°i n·∫øu c√≥
                conversation_context_str = ""
                if conversation_history and conversation_history.strip():
                    conversation_context_str = f"NG·ªÆ C·∫¢NH CU·ªòC H·ªòI THO·∫†I:\n{conversation_history.strip()}\n"
                
                # Format prompt v·ªõi query v√† conversation context
                final_prompt = prompt_template.format(
                    query=query_to_use,
                    conversation_context=conversation_context_str
                )
                
                try:
                    async for content_chunk in self.llm.stream(final_prompt):
                        yield {"type": "content", "data": {"content": content_chunk}}
                except Exception as e:
                    print(f"L·ªói khi g·ªçi LLM stream cho sql_code_task: {str(e)}")
                    yield {
                        "type": "content",
                        "data": {
                            "content": f"Xin l·ªói, c√≥ l·ªói x·∫£y ra khi x·ª≠ l√Ω y√™u c·∫ßu SQL c·ªßa b·∫°n: {str(e)}"
                        },
                    }
            
            elapsed_time = time.time() - start_time
            yield {
                "type": "end",
                "data": {
                    "processing_time": round(elapsed_time, 2),
                    "query_type": "sql_code_task",
                },
            }
            return

        # Tr·∫£ v·ªÅ ngay n·∫øu l√† c√¢u h·ªèi th·ªùi gian th·ª±c
        if query_type == "realtime_question":
            # Tr·∫£ v·ªÅ th√¥ng b√°o b·∫Øt ƒë·∫ßu
            yield {
                "type": "start",
                "data": {
                    "query_type": query_type,
                    "search_type": "google_raw_search",
                    "file_id": file_id,
                },
            }

            # S·ª≠ d·ª•ng Google Search ƒë·ªÉ t√¨m ki·∫øm (k·∫øt qu·∫£ th√¥)
            try:
                # L·∫•y k·∫øt qu·∫£ th√¥ t·ª´ Google Search
                raw_search_content, gas_urls = get_raw_search_results(query_to_use)
                
                # Chu·∫©n b·ªã danh s√°ch ngu·ªìn t·ª´ Google Search
                gas_sources_list = []
                if gas_urls:
                    for url_idx, url in enumerate(gas_urls):
                        gas_sources_list.append({
                            "source": url,  # URL th·ª±c t·∫ø
                            "page": f"Web Search {url_idx+1}",
                            "section": "Online Source",
                            "score": 1.0,
                            "content_snippet": url,  # Hi·ªÉn th·ªã URL l√†m snippet
                            "file_id": "web_search",
                            "is_web_search": True,
                            "url": url  # Th√™m field url ri√™ng
                        })
                
                # Tr·∫£ v·ªÅ ngu·ªìn
                yield {
                    "type": "sources",
                    "data": {
                        "sources": gas_sources_list,
                        "filtered_sources": [],
                        "filtered_file_id": file_id if file_id else [],
                    },
                }

                # S·ª≠ d·ª•ng template REALTIME_QUESTION ƒë·ªÉ t·∫°o prompt
                prompt = self.prompt_manager.get_realtime_question_prompt(
                    query=query_to_use,
                    search_results=raw_search_content,
                    conversation_history=conversation_history
                )

                # G·ªçi LLM ƒë·ªÉ tr·∫£ l·ªùi d∆∞·ªõi d·∫°ng stream
                try:
                    async for content in self.llm.stream(prompt):
                        yield {"type": "content", "data": {"content": content}}
                except Exception as e:
                    print(f"L·ªói khi g·ªçi LLM stream cho realtime_question: {str(e)}")
                    # Tr·∫£ v·ªÅ l·ªói
                    yield {
                        "type": "content",
                        "data": {
                            "content": f"Xin l·ªói, c√≥ l·ªói x·∫£y ra khi x·ª≠ l√Ω c√¢u h·ªèi th·ªùi gian th·ª±c: {str(e)}"
                        },
                    }
            except Exception as e:
                print(f"L·ªói khi s·ª≠ d·ª•ng Google Search (stream): {str(e)}")
                # Tr·∫£ v·ªÅ ngu·ªìn r·ªóng
                yield {
                    "type": "sources",
                    "data": {
                        "sources": [],
                        "filtered_sources": [],
                        "filtered_file_id": file_id if file_id else [],
                    },
                }
                # Tr·∫£ v·ªÅ th√¥ng b√°o l·ªói
                yield {"type": "content", "data": {"content": f"Kh√¥ng th·ªÉ s·ª≠ d·ª•ng Google Search: {str(e)}. Vui l√≤ng ki·ªÉm tra l·∫°i API key ho·∫∑c c·∫•u h√¨nh."}}

            # Tr·∫£ v·ªÅ k·∫øt th√∫c
            elapsed_time = time.time() - start_time
            yield {
                "type": "end",
                "data": {
                    "processing_time": round(elapsed_time, 2),
                    "query_type": query_type,
                },
            }
            return
        RETRIEVAL_K = int(os.getenv("RETRIEVAL_K", "50"))
        
        # Kh·ªüi t·∫°o bi·∫øn ƒë·ªÉ l∆∞u tr·ªØ k·∫øt qu·∫£ Google Search cho question_from_document
        google_search_content = ""
        google_search_urls = []
        google_search_used = False
        
        # N·∫øu l√† question_from_document, th·ª±c hi·ªán search song song
        if query_type == "question_from_document":
            print(f"üîç Th·ª±c hi·ªán t√¨m ki·∫øm song song cho question_from_document: '{query_to_use}'")
            
            # T·∫°o tasks ƒë·ªÉ ch·∫°y song song
            search_tasks = []
            
            # Task 1: Semantic search trong t√†i li·ªáu
            semantic_task = asyncio.create_task(
                self.semantic_search_async(
                    query_to_use, k=RETRIEVAL_K, sources=sources, file_id=file_id
                )
            )
            search_tasks.append(('semantic', semantic_task))
            
            # Task 2: Google Search
            async def google_search_task():
                try:
                    # Ch·∫°y Google Search trong thread pool ƒë·ªÉ tr√°nh blocking
                    loop = asyncio.get_event_loop()
                    content, urls = await loop.run_in_executor(None, get_raw_search_results, query_to_use)
                    return content, urls
                except Exception as e:
                    print(f"L·ªói khi th·ª±c hi·ªán Google Search: {str(e)}")
                    return "", []
            
            google_task = asyncio.create_task(google_search_task())
            search_tasks.append(('google', google_task))
            
            # Ch·ªù t·∫•t c·∫£ tasks ho√†n th√†nh
            completed_tasks = await asyncio.gather(*[task for _, task in search_tasks], return_exceptions=True)
            
            # X·ª≠ l√Ω k·∫øt qu·∫£ semantic search
            search_results = completed_tasks[0] if not isinstance(completed_tasks[0], Exception) else []
            if isinstance(completed_tasks[0], Exception):
                print(f"L·ªói trong semantic search: {completed_tasks[0]}")
                search_results = []
            
            # X·ª≠ l√Ω k·∫øt qu·∫£ Google search
            if not isinstance(completed_tasks[1], Exception):
                google_search_content, google_search_urls = completed_tasks[1]
                if google_search_content and google_search_content != "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan ƒë·∫øn truy v·∫•n n√†y.":
                    google_search_used = True
                    print(f"‚úÖ Google Search ho√†n th√†nh: {len(google_search_urls)} URLs")
                else:
                    print("‚ö†Ô∏è Google Search kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£")
            else:
                print(f"‚ùå L·ªói trong Google Search: {completed_tasks[1]}")
                google_search_content, google_search_urls = "", []
        else:
            # Th·ª±c hi·ªán semantic search th√¥ng th∆∞·ªùng cho c√°c lo·∫°i kh√°c
            search_results = await self.semantic_search_async(
                query_to_use, k=RETRIEVAL_K, sources=sources, file_id=file_id
            )
        
        # Fallback mechanism cho streaming (ch·ªâ √°p d·ª•ng khi kh√¥ng c√≥ k·∫øt qu·∫£ RAG v√† kh√¥ng ph·∫£i question_from_document)
        perform_fallback_stream = not search_results or len(search_results) == 0
        gas_fallback_used = False
        
        if perform_fallback_stream and query_type != "question_from_document":
            print(f"Kh√¥ng c√≥ k·∫øt qu·∫£ RAG (stream). Th·ª±c hi·ªán fallback v·ªõi Google Search cho: '{query_to_use}'")
            try:
                # Kh·ªüi t·∫°o sources_list tr∆∞·ªõc khi s·ª≠ d·ª•ng
                sources_list = []
                # Ch·∫°y Google Search trong thread pool ƒë·ªÉ tr√°nh blocking
                loop = asyncio.get_event_loop()
                fallback_content, fallback_urls = await loop.run_in_executor(None, get_raw_search_results, query_to_use)
                
                if fallback_content and fallback_content != "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan ƒë·∫øn truy v·∫•n n√†y.":
                    gas_fallback_used = True
                    print(f"ƒê√£ t√¨m th·∫•y k·∫øt qu·∫£ fallback: {fallback_content[:100]}...")
                    
                    # T·∫°o m·ªôt doc gi·∫£ cho fallback ƒë·ªÉ ƒë∆∞a v√†o context LLM
                    fallback_doc = {
                        "text": fallback_content,
                        "metadata": {
                            "source": "Google Search",
                            "page": "Web Result",
                            "source_type": "web_search",
                            "urls": fallback_urls
                        },
                        "score": 0.9,
                        "rerank_score": 0.9,
                        "file_id": "web_search_fallback"
                    }
                    
                    # Th√™m v√†o search_results
                    search_results = [fallback_doc]
                    
                    # C·∫≠p nh·∫≠t danh s√°ch ngu·ªìn v·ªõi k·∫øt qu·∫£ fallback
                    for url_idx, url in enumerate(fallback_urls):
                        sources_list.append({
                            "source": url,  # URL th·ª±c t·∫ø
                            "page": f"Fallback Search {url_idx+1}",
                            "section": "Online Source",
                            "score": 0.9,
                            "content_snippet": url,  # Hi·ªÉn th·ªã URL l√†m snippet
                            "file_id": "web_search_fallback",
                            "is_web_search": True,
                            "url": url  # Th√™m field url ri√™ng
                        })
                else:
                    print("Google Search kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ fallback.")
            except Exception as e:
                print(f"L·ªói khi th·ª±c hi·ªán fallback v·ªõi Google Search: {str(e)}")
                # Ti·∫øp t·ª•c v·ªõi k·∫øt qu·∫£ hi·ªán t·∫°i

        # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p kh√¥ng c√≥ k·∫øt qu·∫£ t·ª´ c·∫£ hai ngu·ªìn
        if (not search_results or len(search_results) == 0) and not google_search_used:
            # Tr·∫£ v·ªÅ th√¥ng b√°o b·∫Øt ƒë·∫ßu
            yield {
                "type": "start",
                "data": {
                    "query_type": "no_results",
                    "file_id": file_id,
                },
            }

            # Tr·∫£ v·ªÅ ngu·ªìn r·ªóng
            yield {
                "type": "sources",
                "data": {
                    "sources": [],
                    "filtered_sources": [],
                    "filtered_file_id": file_id if file_id else [],
                },
            }

            # Tr·∫£ v·ªÅ n·ªôi dung
            response = "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan ƒë·∫øn c√¢u h·ªèi c·ªßa b·∫°n trong t√†i li·ªáu v√† c≈©ng kh√¥ng th·ªÉ t√¨m ki·∫øm th√¥ng tin b·ªï sung t·ª´ Internet. Vui l√≤ng th·ª≠ l·∫°i v·ªõi c√¢u h·ªèi kh√°c ho·∫∑c ƒëi·ªÅu ch·ªânh t·ª´ kh√≥a t√¨m ki·∫øm."
            yield {"type": "content", "data": {"content": response}}

            # Tr·∫£ v·ªÅ k·∫øt th√∫c
            elapsed_time = time.time() - start_time
            yield {
                "type": "end",
                "data": {
                    "processing_time": round(elapsed_time, 2),
                    "query_type": "no_results",
                },
            }
            return

        # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát: ch·ªâ c√≥ Google Search results (kh√¥ng c√≥ document results)
        if (not search_results or len(search_results) == 0) and google_search_used and query_type == "question_from_document":
            print(f"‚ö†Ô∏è Ch·ªâ c√≥ k·∫øt qu·∫£ t·ª´ Google Search, kh√¥ng c√≥ k·∫øt qu·∫£ t·ª´ t√†i li·ªáu cho question_from_document")
            # T·∫°o context tr·ªëng cho t√†i li·ªáu
            context_docs = []
            
            # T·∫°o sources list ch·ªâ t·ª´ Google Search
            sources_list = []
            for url_idx, url in enumerate(google_search_urls):
                sources_list.append({
                    "source": url,
                    "page": f"Internet Search {url_idx+1}",
                    "section": "Online Source",
                    "score": 0.8,
                    "content_snippet": url,
                    "file_id": "internet_search",
                    "is_web_search": True,
                    "url": url,
                    "source_type": "internet"
                })

            # Tr·∫£ v·ªÅ th√¥ng b√°o b·∫Øt ƒë·∫ßu
            yield {
                "type": "start",
                "data": {
                    "query_type": query_type,
                    "file_id": file_id,
                    "total_results": 0,
                    "total_reranked": 0,
                    "google_search_used": True,
                    "google_search_urls_count": len(google_search_urls),
                    "only_internet_results": True,
                },
            }

            # Tr·∫£ v·ªÅ ngu·ªìn tham kh·∫£o
            yield {
                "type": "sources",
                "data": {
                    "sources": sources_list,
                    "filtered_sources": [],
                    "filtered_file_id": file_id if file_id else [],
                    "google_search_used": True,
                },
            }

            # S·ª≠ d·ª•ng prompt ƒë·∫∑c bi·ªát v·ªõi ch·ªâ c√≥ Google search content
            prompt = self.prompt_manager.create_prompt_with_google_search(
                query_to_use, 
                [],  # Context tr·ªëng
                google_search_content,
                conversation_history=conversation_history
            )

            # G·ªçi LLM ƒë·ªÉ tr·∫£ l·ªùi
            try:
                async for content in self.llm.stream(prompt):
                    yield {"type": "content", "data": {"content": content}}
            except Exception as e:
                print(f"L·ªói khi g·ªçi LLM stream: {str(e)}")
                yield {
                    "type": "content",
                    "data": {
                        "content": f"Xin l·ªói, c√≥ l·ªói x·∫£y ra khi x·ª≠ l√Ω c√¢u h·ªèi: {str(e)}"
                    },
                }

            # K·∫øt th√∫c ƒëo th·ªùi gian
            elapsed_time = time.time() - start_time

            # Tr·∫£ v·ªÅ k·∫øt th√∫c
            yield {
                "type": "end",
                "data": {
                    "processing_time": round(elapsed_time, 2),
                    "query_type": query_type,
                    "google_search_used": True,
                    "only_internet_results": True,
                },
            }
            return
        RERANK_TOP_N = int(os.getenv("RERANK_TOP_N", "15"))
        results_to_rerank = search_results[:RERANK_TOP_N] if search_results else []
        print(f"L·∫•y v·ªÅ {len(search_results) if search_results else 0} k·∫øt qu·∫£, s·∫Ω rerank top {len(results_to_rerank)}.")

        # Rerank k·∫øt qu·∫£ n·∫øu c√≥ nhi·ªÅu h∆°n 1 k·∫øt qu·∫£
        if len(search_results) > 0:
            reranked_results = await self.rerank_results_async(
                query_to_use, results_to_rerank
            )
            # L·∫•y s·ªë l∆∞·ª£ng k·∫øt qu·∫£ ƒë√£ rerank
            total_reranked = len(reranked_results)
        else:
            reranked_results = search_results if search_results else []
            total_reranked = len(reranked_results)

        # Chu·∫©n b·ªã context t·ª´ c√°c k·∫øt qu·∫£ ƒë√£ rerank
        context_docs = []
        for i, result in enumerate(reranked_results[:20]):  
            # Chu·∫©n b·ªã metadata
            metadata = result.get("metadata", {})
            source = metadata.get("source", "unknown")
            
            # ∆Øu ti√™n s·ª≠ d·ª•ng page_label n·∫øu c√≥
            page = metadata.get("page_label", metadata.get("page", "N/A"))
            section = metadata.get("section", "N/A")

            # Th√™m v√†o danh s√°ch context
            context_docs.append(
                {
                    "content": result["text"],
                    "source": source,
                    "page": page,
                    "page_label": metadata.get("page_label", ""),  # Th√™m page_label v√†o context
                    "section": section,
                    "score": result.get("score", 0.0),
                    "metadata": metadata,  # Th√™m to√†n b·ªô metadata ƒë·ªÉ s·ª≠ d·ª•ng trong prompt
                }
            )

        # Chu·∫©n b·ªã danh s√°ch ngu·ªìn tham kh·∫£o
        sources_list = []
        
        # Th√™m ngu·ªìn t·ª´ RAG documents
        for i, doc in enumerate(reranked_results):
            # Tr√≠ch xu·∫•t th√¥ng tin t·ª´ metadata
            metadata = doc.get("metadata", {})
            
            # Ki·ªÉm tra n·∫øu l√† ngu·ªìn t·ª´ Google Search
            if metadata.get("source_type") == "web_search":
                urls_from_gas = metadata.get("urls", [])
                snippet = doc["text"]
                if urls_from_gas:
                    snippet += "\n\nNgu·ªìn tham kh·∫£o t·ª´ web:\n" + "\n".join([f"- {url}" for url in urls_from_gas])
                
                sources_list.append({
                    "source": "Google Search",
                    "page": "Web Search Result",
                    "section": "Web Content",
                    "score": doc.get("score", 0.9),
                    "content_snippet": snippet,
                    "file_id": doc.get("file_id", "web_search"),
                    "is_web_search": True
                })
            else:
                # Ngu·ªìn t·ª´ RAG th√¥ng th∆∞·ªùng
                source = metadata.get("source", "unknown")
                
                # ∆Øu ti√™n s·ª≠ d·ª•ng page_label n·∫øu c√≥
                page = metadata.get("page_label", metadata.get("page", "N/A"))
                page_label = metadata.get("page_label", "")
                section = metadata.get("section", "N/A")
                result_file_id = doc.get("file_id", "unknown")  # L·∫•y file_id t·ª´ k·∫øt qu·∫£

                # T·∫°o snippet t·ª´ n·ªôi dung
                content = doc["text"]
                snippet = content

                # Th√™m v√†o danh s√°ch ngu·ªìn
                sources_list.append(
                    {
                        "source": source,
                        "page": page,
                        "page_label": page_label,  # Th√™m page_label v√†o sources
                        "section": section,
                        "score": doc.get("score", 0.0),
                        "content_snippet": snippet,
                        "file_id": result_file_id,
                        "is_web_search": False,
                        "source_filename": os.path.basename(source) if os.path.sep in source else source,  # Th√™m t√™n file kh√¥ng c√≥ ƒë∆∞·ªùng d·∫´n
                    }
                )

        # Th√™m ngu·ªìn t·ª´ Google Search n·∫øu c√≥ (cho question_from_document)
        if google_search_used and query_type == "question_from_document":
            for url_idx, url in enumerate(google_search_urls):
                sources_list.append({
                    "source": url,
                    "page": f"Internet Search {url_idx+1}",
                    "section": "Online Source",
                    "score": 0.8,  # ƒêi·ªÉm th·∫•p h∆°n ngu·ªìn t√†i li·ªáu
                    "content_snippet": url,
                    "file_id": "internet_search",
                    "is_web_search": True,
                    "url": url,
                    "source_type": "internet"
                })

        # Tr·∫£ v·ªÅ th√¥ng b√°o b·∫Øt ƒë·∫ßu
        yield {
            "type": "start",
            "data": {
                "query_type": query_type,
                "file_id": file_id,
                "total_results": len(search_results) if search_results else 0,
                "total_reranked": total_reranked,
                "google_search_used": google_search_used,
                "google_search_urls_count": len(google_search_urls) if google_search_used else 0,
            },
        }

        # Tr·∫£ v·ªÅ ngu·ªìn tham kh·∫£o
        yield {
            "type": "sources",
            "data": {
                "sources": sources_list,
                "filtered_sources": [],  # Gi·ªØ tr∆∞·ªùng n√†y ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi code c≈©
                "filtered_file_id": file_id if file_id else [],
                "google_search_used": google_search_used,
            },
        }

        # Chu·∫©n b·ªã prompt cho LLM v·ªõi c·∫£ ngu·ªìn t√†i li·ªáu v√† Google search
        if query_type == "question_from_document" and google_search_used:
            # S·ª≠ d·ª•ng prompt ƒë·∫∑c bi·ªát cho question_from_document c√≥ k·∫øt h·ª£p Google search
            prompt = self.prompt_manager.create_prompt_with_google_search(
                query_to_use, 
                context_docs, 
                google_search_content,
                conversation_history=conversation_history
            )
        else:
            # S·ª≠ d·ª•ng prompt th√¥ng th∆∞·ªùng
            prompt = self.prompt_manager.create_prompt_with_history(
                query_to_use, context_docs, conversation_history=conversation_history
            )

        # G·ªçi LLM ƒë·ªÉ tr·∫£ l·ªùi
        try:
            # S·ª≠ d·ª•ng LLM ƒë·ªÉ tr·∫£ l·ªùi d∆∞·ªõi d·∫°ng stream
            async for content in self.llm.stream(prompt):
                yield {"type": "content", "data": {"content": content}}
        except Exception as e:
            print(f"L·ªói khi g·ªçi LLM stream: {str(e)}")
            # Tr·∫£ v·ªÅ l·ªói
            yield {
                "type": "content",
                "data": {
                    "content": f"Xin l·ªói, c√≥ l·ªói x·∫£y ra khi x·ª≠ l√Ω c√¢u h·ªèi: {str(e)}"
                },
            }

        # K·∫øt th√∫c ƒëo th·ªùi gian
        elapsed_time = time.time() - start_time

        # Tr·∫£ v·ªÅ k·∫øt th√∫c
        yield {
            "type": "end",
            "data": {
                "processing_time": round(elapsed_time, 2),
                "query_type": query_type,
                "google_search_used": google_search_used,
            },
        }

    def delete_collection(self) -> None:
        """X√≥a collection"""
        self.vector_store.delete_collection()

    def get_collection_info(self) -> Dict:
        """L·∫•y th√¥ng tin v·ªÅ collection"""
        return self.vector_store.get_collection_info()

    async def generate_related_questions(self, query: str, answer: str) -> List[str]:
        """T·∫°o danh s√°ch c√°c c√¢u h·ªèi g·ª£i √Ω li√™n quan s·ª≠ d·ª•ng SuggestionManager"""
        try:
            # T·∫°o conversation context t·ª´ Q&A hi·ªán t·∫°i
            conversation_context = f"Ng∆∞·ªùi d√πng: {query}\n\nTr·ª£ l√Ω: {answer}"
            
            # S·ª≠ d·ª•ng SuggestionManager thay v√¨ template
            suggestions = await self.suggestion_manager.generate_question_suggestions(
                conversation_context, num_suggestions=3
            )
            return suggestions[:3]  # ƒê·∫£m b·∫£o ch·ªâ tr·∫£ v·ªÅ 3 c√¢u h·ªèi
            
        except Exception as e:
            print(f"L·ªói khi t·∫°o c√¢u h·ªèi li√™n quan: {str(e)}")
            # Tr·∫£ v·ªÅ c√¢u h·ªèi m·∫∑c ƒë·ªãnh
            return [
                "B·∫°n mu·ªën t√¨m hi·ªÉu th√™m ƒëi·ªÅu g√¨ v·ªÅ ch·ªß ƒë·ªÅ n√†y?",
                "B·∫°n c√≥ th·∫Øc m·∫Øc n√†o kh√°c li√™n quan ƒë·∫øn n·ªôi dung n√†y kh√¥ng?",
                "B·∫°n c√≥ mu·ªën bi·∫øt th√™m th√¥ng tin v·ªÅ ·ª©ng d·ª•ng th·ª±c t·∫ø kh√¥ng?",
            ]

    # def _generate_answer(self, query, relevant_docs, **kwargs):
    #     """Ph∆∞∆°ng th·ª©c n·ªôi b·ªô ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi"""
    #     # T·∫°o context t·ª´ c√°c t√†i li·ªáu li√™n quan
    #     context = "\n---\n".join([doc["text"] for doc in relevant_docs])

    #     # T·∫°o prompt v·ªõi template ph√π h·ª£p
    #     prompt = self.prompt_manager.templates["query_with_context"].format(
    #         context=context, query=query
    #     ) # G·ªçi LLM v√† l·∫•y k·∫øt qu·∫£
    #     response = self.llm.invoke(prompt)
    #     return response.content

