import logging
import asyncio

# C·∫•u h√¨nh logging cho c·∫£ logging v√† print
logging.basicConfig(format="[RAG_Pipeline] %(message)s", level=logging.INFO)
logger = logging.getLogger(__name__)

# Ghi ƒë√® h√†m print ƒë·ªÉ th√™m prefix
original_print = print


def print(*args, **kwargs):
    prefix = "[RAG_Pipeline] "
    original_print(prefix + " ".join(map(str, args)), **kwargs)


# C·∫•u h√¨nh logging
logging.basicConfig(format="[RAG_Pipeline] %(message)s", level=logging.INFO)
logger = logging.getLogger(__name__)

from typing import List, Dict, AsyncGenerator
from backend.embedding import EmbeddingModel
from backend.llm import GeminiLLM
from backend.vector_store import VectorStore
from backend.document_processor import DocumentProcessor
from backend.prompt_manager import PromptManager
from backend.search import SearchManager
from backend.suggestion_manager import SuggestionManager
# from backend.query_processor import QueryProcessor
# from backend.query_router import QueryRouter
from backend.query_handler import QueryHandler
import os
import re
import concurrent.futures
from dotenv import load_dotenv
import time
import asyncio
import requests
import uuid
import json

# Import Google_Search
from backend.tools.Google_Search import get_raw_search_results

# Load bi·∫øn m√¥i tr∆∞·ªùng t·ª´ .env
load_dotenv()

# Khai b√°o c√°c bi·∫øn to√†n c·ª•c cho c√°c t√†i nguy√™n d√πng chung
global_embedding_model = None
global_llm_model = None
global_document_processor = None
global_prompt_manager = None
global_search_manager = None
global_resources_initialized = False


def initialize_global_resources():
    """Kh·ªüi t·∫°o c√°c t√†i nguy√™n d√πng chung to√†n c·ª•c m·ªôt l·∫ßn duy nh·∫•t"""
    global global_embedding_model, global_llm_model, global_document_processor, global_prompt_manager, global_search_manager, global_resources_initialized

    if not global_resources_initialized:
        print("B·∫Øt ƒë·∫ßu kh·ªüi t·∫°o c√°c t√†i nguy√™n to√†n c·ª•c...")

        # Kh·ªüi t·∫°o c√°c model ch·ªâ m·ªôt l·∫ßn
        global_embedding_model = EmbeddingModel()
        print("ƒê√£ kh·ªüi t·∫°o embedding model to√†n c·ª•c")

        global_llm_model = GeminiLLM()
        print("ƒê√£ kh·ªüi t·∫°o LLM to√†n c·ª•c")

        global_document_processor = DocumentProcessor()
        print("ƒê√£ kh·ªüi t·∫°o Document Processor to√†n c·ª•c")

        global_prompt_manager = PromptManager()
        print("ƒê√£ kh·ªüi t·∫°o Prompt Manager to√†n c·ª•c")

        # Search manager s·∫Ω ƒë∆∞·ª£c t·∫°o ri√™ng cho t·ª´ng user ƒë·ªÉ tr√°nh conflict
        # Kh√¥ng t·∫°o global search manager ƒë·ªÉ tr√°nh v·∫•n ƒë·ªÅ v·ªõi BM25 index
        global_search_manager = None
        print("Search Manager s·∫Ω ƒë∆∞·ª£c t·∫°o ri√™ng cho t·ª´ng user khi c·∫ßn thi·∫øt")

        global_resources_initialized = True
        print("Ho√†n th√†nh kh·ªüi t·∫°o t·∫•t c·∫£ t√†i nguy√™n to√†n c·ª•c")
    else:
        print("C√°c t√†i nguy√™n to√†n c·ª•c ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o tr∆∞·ªõc ƒë√≥")

    return {
        "embedding_model": global_embedding_model,
        "llm_model": global_llm_model,
        "document_processor": global_document_processor,
        "prompt_manager": global_prompt_manager,
        "search_manager": global_search_manager,
    }


class AdvancedDatabaseRAG:
    """L·ªõp ch√≠nh k·∫øt h·ª£p t·∫•t c·∫£ c√°c th√†nh ph·∫ßn c·ªßa h·ªá th·ªëng RAG"""

    def __init__(
        self,
        api_key=None,
        user_id=None,
        embedding_model=None,
        llm_model=None,
        document_processor=None,
        prompt_manager=None,
        search_manager=None,
    ):
        """Kh·ªüi t·∫°o h·ªá th·ªëng RAG"""
        # Kh·ªüi t·∫°o c√°c th√†nh ph·∫ßn ri√™ng bi·ªát t·ª´ b√™n ngo√†i ho·∫∑c t·∫°o m·ªõi
        if embedding_model is not None:
            self.embedding_model = embedding_model
            print("S·ª≠ d·ª•ng embedding model ƒë∆∞·ª£c cung c·∫•p t·ª´ b√™n ngo√†i")
        else:
            print("Kh·ªüi t·∫°o embedding model m·ªõi")
            self.embedding_model = EmbeddingModel()

        if llm_model is not None:
            self.llm = llm_model
            print("S·ª≠ d·ª•ng LLM ƒë∆∞·ª£c cung c·∫•p t·ª´ b√™n ngo√†i")
        else:
            print("Kh·ªüi t·∫°o LLM m·ªõi")
            self.llm = GeminiLLM(api_key)

        # L∆∞u tr·ªØ user_id
        self.user_id = user_id
        # Kh·ªüi t·∫°o vector store v·ªõi user_id
        self.vector_store = VectorStore(user_id=user_id)
        print(f"Kh·ªüi t·∫°o h·ªá th·ªëng RAG cho user_id={user_id}")

        if document_processor is not None:
            self.document_processor = document_processor
            print("S·ª≠ d·ª•ng Document Processor ƒë∆∞·ª£c cung c·∫•p t·ª´ b√™n ngo√†i")
        else:
            print("Kh·ªüi t·∫°o Document Processor m·ªõi")
            self.document_processor = DocumentProcessor()

        if prompt_manager is not None:
            self.prompt_manager = prompt_manager
            print("S·ª≠ d·ª•ng Prompt Manager ƒë∆∞·ª£c cung c·∫•p t·ª´ b√™n ngo√†i")
        else:
            print("Kh·ªüi t·∫°o Prompt Manager m·ªõi")
            self.prompt_manager = PromptManager()

        # S·ª≠ d·ª•ng search_manager t·ª´ b√™n ngo√†i ho·∫∑c t·∫°o m·ªõi v·ªõi vector_store c·ªßa user
        if search_manager is not None:
            # G√°n search_manager to√†n c·ª•c v√† c·∫≠p nh·∫≠t vector_store (c√πng v·ªõi BM25 index t∆∞∆°ng ·ª©ng)
            self.search_manager = search_manager
            # # Thay v√¨ ch·ªâ g√°n vector_store, g·ªçi ph∆∞∆°ng th·ª©c ƒë·ªÉ c·∫≠p nh·∫≠t v√† t·∫£i l·∫°i BM25 index ph√π h·ª£p
            # self.search_manager.set_vector_store_and_reload_bm25(self.vector_store)
            # print("S·ª≠ d·ª•ng Search Manager to√†n c·ª•c (ƒë√£ c·∫≠p nh·∫≠t vector_store v√† BM25 index cho user)")
        else:
            print("Kh·ªüi t·∫°o Search Manager m·ªõi")
            self.search_manager = SearchManager(self.vector_store, self.embedding_model)

        # # Th√™m QueryProcessor cho vi·ªác x·ª≠ l√Ω ƒë·ªìng tham chi·∫øu
        # self.query_processor = QueryProcessor()
        # print("ƒê√£ kh·ªüi t·∫°o QueryProcessor v·ªõi kh·∫£ nƒÉng x·ª≠ l√Ω ƒë·ªìng tham chi·∫øu")

        # # Th√™m QueryRouter cho vi·ªác ph√¢n lo·∫°i c√¢u h·ªèi
        # self.query_router = QueryRouter()
        # print("ƒê√£ kh·ªüi t·∫°o QueryRouter ƒë·ªÉ ph√¢n lo·∫°i c√¢u h·ªèi th√†nh 3 lo·∫°i")

                # H·ª£p nh·∫•t QueryProcessor v√† QueryRouter th√†nh QueryHandler
        self.query_handler = QueryHandler()
        print("ƒê√£ kh·ªüi t·∫°o QueryHandler ƒë·ªÉ x·ª≠ l√Ω v√† ph√¢n lo·∫°i c√¢u h·ªèi")

        # Kh·ªüi t·∫°o SuggestionManager cho vi·ªác t·∫°o c√¢u h·ªèi li√™n quan
        self.suggestion_manager = SuggestionManager(llm=self.llm)
        print("ƒê√£ kh·ªüi t·∫°o SuggestionManager ƒë·ªÉ t·∫°o c√¢u h·ªèi g·ª£i √Ω")

        # T√≠nh to√°n v√† l∆∞u c√°c gi√° tr·ªã kh√°c n·∫øu c·∫ßn
        self.enable_fact_checking = (
            False  # T√≠nh nƒÉng ki·ªÉm tra s·ª± ki·ªán (c√≥ th·ªÉ k√≠ch ho·∫°t sau)
        )

        self.confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", "0.5"))

        # C√†i ƒë·∫∑t c∆° ch·∫ø x·ª≠ l√Ω song song
        self.max_workers = int(os.getenv("MAX_PARALLEL_WORKERS", "4"))

        # **TH√äM: Config cho Smart Fallback feature**
        self.enable_smart_fallback = os.getenv("ENABLE_SMART_FALLBACK", "true").lower() == "true"
        self.context_quality_threshold = float(os.getenv("CONTEXT_QUALITY_THRESHOLD", "0.2"))
        
        # **TH√äM: Config cho Response-based Fallback feature**
        self.enable_response_fallback = os.getenv("ENABLE_RESPONSE_FALLBACK", "true").lower() == "true"
        
        print(f"Smart Fallback: {'Enabled' if self.enable_smart_fallback else 'Disabled'} (threshold: {self.context_quality_threshold})")
        print(f"Response-based Fallback: {'Enabled' if self.enable_response_fallback else 'Disabled'}")

    async def _evaluate_context_quality(self, query: str, context_docs: List[Dict]) -> Dict[str, any]:
        """
        ƒê√°nh gi√° xem context c√≥ ƒë·ªß th√¥ng tin ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi kh√¥ng
        
        Args:
            query: C√¢u h·ªèi ng∆∞·ªùi d√πng
            context_docs: Danh s√°ch c√°c document context
            
        Returns:
            Dict v·ªõi c√°c key: {"is_sufficient": bool, "confidence": float, "reason": str}
        """
        if not context_docs:
            return {"is_sufficient": False, "confidence": 0.0, "reason": "No context documents"}
        
        # T·∫°o context preview ng·∫Øn g·ªçn cho evaluation (ch·ªâ l·∫•y 200 k√Ω t·ª± ƒë·∫ßu c·ªßa m·ªói doc)
        context_preview = "\n---\n".join([
            f"Doc {i+1}: {doc.get('content', '')[:200]}..." 
            for i, doc in enumerate(context_docs[:3])  # Ch·ªâ l·∫•y 3 docs ƒë·∫ßu
        ])
        
        evaluation_prompt = f"""B·∫°n l√† chuy√™n gia ƒë√°nh gi√° th√¥ng tin. H√£y ƒë√°nh gi√° xem th√¥ng tin d∆∞·ªõi ƒë√¢y c√≥ ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi kh√¥ng.

C√ÇU H·ªéI: {query}

TH√îNG TIN HI·ªÜN C√ì:
{context_preview}

H√£y tr·∫£ v·ªÅ JSON v·ªõi ƒë·ªãnh d·∫°ng ch√≠nh x√°c:
{{"is_sufficient": true/false, "confidence": 0.0-1.0, "reason": "l√Ω do ng·∫Øn g·ªçn"}}

NGUY√äN T·∫ÆC ƒê√ÅNH GI√Å:
- is_sufficient: true n·∫øu th√¥ng tin ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi √≠t nh·∫•t 70% c√¢u h·ªèi
- confidence: 0.0-1.0 (0.7+ l√† ƒë·ªß t·ªët, 0.5-0.69 l√† trung b√¨nh, <0.5 l√† kh√¥ng ƒë·ªß)
- reason: gi·∫£i th√≠ch ng·∫Øn g·ªçn (t·ªëi ƒëa 50 t·ª´)

V√ç D·ª§:
- N·∫øu c√¢u h·ªèi v·ªÅ "kh√≥a ch√≠nh SQL" v√† document c√≥ ƒë·∫ßy ƒë·ªß ƒë·ªãnh nghƒ©a, c√∫ ph√°p ‚Üí {{"is_sufficient": true, "confidence": 0.9, "reason": "C√≥ ƒë·∫ßy ƒë·ªß th√¥ng tin v·ªÅ kh√≥a ch√≠nh"}}
- N·∫øu c√¢u h·ªèi v·ªÅ "PostgreSQL 16" nh∆∞ng document ch·ªâ n√≥i v·ªÅ MySQL ‚Üí {{"is_sufficient": false, "confidence": 0.2, "reason": "Document kh√¥ng ch·ª©a th√¥ng tin v·ªÅ PostgreSQL"}}

CH·ªà tr·∫£ v·ªÅ JSON, kh√¥ng c√≥ text kh√°c."""

        try:
            response = await self.llm.invoke(evaluation_prompt)
            response_text = response.content.strip()
            
            # T√¨m JSON trong response
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                result = json.loads(json_str)
                
                # Validate response format
                if not all(key in result for key in ["is_sufficient", "confidence", "reason"]):
                    print("‚ö†Ô∏è Invalid evaluation response format")
                    return {"is_sufficient": True, "confidence": 0.6, "reason": "Evaluation format error"}
                
                # ƒê·∫£m b·∫£o confidence l√† float trong kho·∫£ng 0-1
                try:
                    confidence = float(result["confidence"])
                    confidence = max(0.0, min(1.0, confidence))  # Clamp between 0-1
                    result["confidence"] = confidence
                except (ValueError, TypeError):
                    result["confidence"] = 0.5
                
                print(f"üìä Context evaluation: sufficient={result['is_sufficient']}, confidence={result['confidence']:.2f}, reason='{result['reason']}'")
                return result
            else:
                print("‚ö†Ô∏è No JSON found in evaluation response")
                return {"is_sufficient": True, "confidence": 0.6, "reason": "JSON parse error"}
                
        except json.JSONDecodeError as e:
            print(f"‚ö†Ô∏è JSON decode error in evaluation: {e}")
            return {"is_sufficient": True, "confidence": 0.6, "reason": "JSON decode error"}
        except Exception as e:
            print(f"‚ö†Ô∏è Error evaluating context quality: {e}")
            # Conservative fallback - n·∫øu kh√¥ng ƒë√°nh gi√° ƒë∆∞·ª£c th√¨ coi nh∆∞ ƒë·ªß ƒë·ªÉ tr√°nh loop
            return {"is_sufficient": True, "confidence": 0.6, "reason": "Evaluation failed"}

    async def _execute_google_fallback_streaming(self, query: str, conversation_history: str, start_time: float, query_type: str, file_id: List[str] = None):
        """
        Execute Google Search fallback v·ªõi streaming response
        
        Args:
            query: C√¢u h·ªèi ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
            conversation_history: L·ªãch s·ª≠ h·ªôi tho·∫°i  
            start_time: Th·ªùi gian b·∫Øt ƒë·∫ßu process
            query_type: Lo·∫°i c√¢u h·ªèi g·ªëc
            file_id: Danh s√°ch file_id (ƒë·ªÉ tr·∫£ v·ªÅ trong response)
        """
        print(f"üîÑ Executing Google Search fallback for: '{query[:50]}...'")
        
        try:
            # G·ªçi Google Search
            fallback_content, fallback_urls = get_raw_search_results(query)
            
            if fallback_content and fallback_content != "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan ƒë·∫øn truy v·∫•n n√†y.":
                print(f"‚úÖ Google fallback found results: {len(fallback_urls)} sources")
                
                # Chu·∫©n b·ªã sources list cho Google Search
                gas_sources_list = []
                for url_idx, url in enumerate(fallback_urls):
                    gas_sources_list.append({
                        "source": url,
                        "page": f"Web Search {url_idx+1}",
                        "section": "Online Source",
                        "score": 0.9,
                        "content_snippet": url,
                        "file_id": "web_search_fallback",
                        "is_web_search": True,
                        "url": url
                    })
                
                # Yield start v·ªõi fallback indicator
                yield {
                    "type": "start",
                    "data": {
                        "query_type": f"{query_type}_smart_fallback",
                        "search_type": "google_smart_fallback", 
                        "fallback_reason": "Insufficient document context",
                        "file_id": file_id if file_id else []
                    }
                }
                
                # Yield sources t·ª´ Google Search
                yield {
                    "type": "sources", 
                    "data": {
                        "sources": gas_sources_list, 
                        "filtered_sources": [], 
                        "filtered_file_id": file_id if file_id else []
                    }
                }
                
                # T·∫°o prompt cho realtime question
                prompt = self.prompt_manager.get_realtime_question_prompt(
                    query=query, 
                    search_results=fallback_content, 
                    conversation_history=conversation_history
                )
                
                # Stream response t·ª´ LLM
                try:
                    async for content in self.llm.stream(prompt):
                        yield {"type": "content", "data": {"content": content}}
                except Exception as llm_error:
                    print(f"‚ùå LLM error in fallback: {llm_error}")
                    yield {"type": "content", "data": {"content": f"L·ªói khi x·ª≠ l√Ω ph·∫£n h·ªìi t·ª´ Google Search: {str(llm_error)}"}}
                
            else:
                print("‚ö†Ô∏è Google fallback returned no useful results")
                # Yield empty sources
                yield {
                    "type": "start",
                    "data": {
                        "query_type": f"{query_type}_failed_fallback",
                        "file_id": file_id if file_id else []
                    }
                }
                yield {
                    "type": "sources", 
                    "data": {"sources": [], "filtered_sources": [], "filtered_file_id": file_id if file_id else []}
                }
                yield {"type": "content", "data": {"content": "Kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p t·ª´ c·∫£ t√†i li·ªáu v√† t√¨m ki·∫øm web. Vui l√≤ng th·ª≠ l·∫°i v·ªõi c√¢u h·ªèi kh√°c ho·∫∑c t·ª´ kh√≥a c·ª• th·ªÉ h∆°n."}}
                
        except Exception as e:
            print(f"‚ùå Error in Google fallback: {e}")
            # Yield error response
            yield {
                "type": "start",
                "data": {
                    "query_type": f"{query_type}_error_fallback",
                    "file_id": file_id if file_id else []
                }
            }
            yield {
                "type": "sources", 
                "data": {"sources": [], "filtered_sources": [], "filtered_file_id": file_id if file_id else []}
            }
            yield {"type": "content", "data": {"content": f"L·ªói khi t√¨m ki·∫øm th√¥ng tin b·ªï sung: {str(e)}"}}
        
        # Yield end
        elapsed_time = time.time() - start_time
        yield {
            "type": "end",
            "data": {
                "processing_time": round(elapsed_time, 2),
                "query_type": f"{query_type}_fallback"
            }
        }

    async def load_documents_async(self, data_dir: str) -> List[Dict]:
        """T·∫£i t√†i li·ªáu t·ª´ th∆∞ m·ª•c (b·∫•t ƒë·ªìng b·ªô)"""
        return await self.document_processor.load_documents(data_dir)

    def load_documents(self, data_dir: str) -> List[Dict]:
        """T·∫£i t√†i li·ªáu t·ª´ th∆∞ m·ª•c (ƒë·ªìng b·ªô)"""
        return self.document_processor.load_documents_sync(data_dir)

    async def process_documents_async(self, documents: List[Dict]) -> List[Dict]:
        """X·ª≠ l√Ω v√† chia nh·ªè t√†i li·ªáu v·ªõi x·ª≠ l√Ω song song (b·∫•t ƒë·ªìng b·ªô)"""
        # X·ª≠ l√Ω song song n·∫øu c√≥ nhi·ªÅu t√†i li·ªáu
        if len(documents) > 5:  # Ch·ªâ x·ª≠ l√Ω song song khi c√≥ nhi·ªÅu t√†i li·ªáu
            print(
                f"X·ª≠ l√Ω song song {len(documents)} t√†i li·ªáu v·ªõi {self.max_workers} workers"
            )
            chunks = []

            # S·ª≠ d·ª•ng asyncio ƒë·ªÉ x·ª≠ l√Ω song song
            tasks = []
            semaphore = asyncio.Semaphore(self.max_workers)
            
            async def process_with_semaphore(doc):
                async with semaphore:
                    return await self._process_single_document_async(doc)
            
            for doc in documents:
                task = asyncio.create_task(process_with_semaphore(doc))
                tasks.append(task)
            
            # Ch·ªù t·∫•t c·∫£ tasks ho√†n th√†nh
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    print(f"L·ªói khi x·ª≠ l√Ω t√†i li·ªáu {i}: {str(result)}")
                else:
                    chunks.extend(result)

            print(f"ƒê√£ x·ª≠ l√Ω xong {len(documents)} t√†i li·ªáu, t·∫°o ra {len(chunks)} chunks")
            return chunks

        else:
            # X·ª≠ l√Ω tu·∫ßn t·ª± cho √≠t t√†i li·ªáu
            print(f"X·ª≠ l√Ω tu·∫ßn t·ª± {len(documents)} t√†i li·ªáu")
            all_chunks = []
            for doc in documents:
                chunks = await self._process_single_document_async(doc)
                all_chunks.extend(chunks)

            print(f"ƒê√£ x·ª≠ l√Ω xong {len(documents)} t√†i li·ªáu, t·∫°o ra {len(all_chunks)} chunks")
            return all_chunks

    def process_documents(self, documents: List[Dict]) -> List[Dict]:
        """X·ª≠ l√Ω v√† chia nh·ªè t√†i li·ªáu v·ªõi x·ª≠ l√Ω song song (ƒë·ªìng b·ªô)"""
        # X·ª≠ l√Ω song song n·∫øu c√≥ nhi·ªÅu t√†i li·ªáu
        if len(documents) > 5:  # Ch·ªâ x·ª≠ l√Ω song song khi c√≥ nhi·ªÅu t√†i li·ªáu
            print(
                f"X·ª≠ l√Ω song song {len(documents)} t√†i li·ªáu v·ªõi {self.max_workers} workers"
            )
            chunks = []

            # S·ª≠ d·ª•ng ThreadPoolExecutor ƒë·ªÉ x·ª≠ l√Ω song song
            with concurrent.futures.ThreadPoolExecutor(
                max_workers=self.max_workers
            ) as executor:
                # Submit t√°c v·ª• x·ª≠ l√Ω cho t·ª´ng t√†i li·ªáu
                future_to_doc = {
                    executor.submit(self._process_single_document, doc): i
                    for i, doc in enumerate(documents)
                }

                # Thu th·∫≠p k·∫øt qu·∫£ khi ho√†n th√†nh
                for future in concurrent.futures.as_completed(future_to_doc):
                    doc_index = future_to_doc[future]
                    try:
                        doc_chunks = future.result()
                        chunks.extend(doc_chunks)
                        print(f"ƒê√£ x·ª≠ l√Ω xong t√†i li·ªáu {doc_index}")
                    except Exception as exc:
                        print(f"L·ªói khi x·ª≠ l√Ω t√†i li·ªáu {doc_index}: {exc}")

            print(f"ƒê√£ x·ª≠ l√Ω xong {len(documents)} t√†i li·ªáu, t·∫°o ra {len(chunks)} chunks")
            return chunks

        else:
            # X·ª≠ l√Ω tu·∫ßn t·ª± cho √≠t t√†i li·ªáu
            print(f"X·ª≠ l√Ω tu·∫ßn t·ª± {len(documents)} t√†i li·ªáu")
            all_chunks = []
            for doc in documents:
                chunks = self._process_single_document(doc)
                all_chunks.extend(chunks)

            print(f"ƒê√£ x·ª≠ l√Ω xong {len(documents)} t√†i li·ªáu, t·∫°o ra {len(all_chunks)} chunks")
            return all_chunks

    async def _process_single_document_async(self, document: Dict) -> List[Dict]:
        """X·ª≠ l√Ω m·ªôt t√†i li·ªáu ƒë∆°n l·∫ª (b·∫•t ƒë·ªìng b·ªô)"""
        return await asyncio.get_event_loop().run_in_executor(
            None, self._process_single_document, document
        )

    def _process_single_document(self, document: Dict) -> List[Dict]:
        """X·ª≠ l√Ω m·ªôt t√†i li·ªáu ƒë∆°n l·∫ª"""
        return self.document_processor.chunk_documents_sync([document])

    async def index_to_qdrant_async(self, chunks: List[Dict], user_id) -> None:
        """Index chunks l√™n Qdrant (b·∫•t ƒë·ªìng b·ªô)"""
        if not chunks:
            print("Kh√¥ng c√≥ chunks n√†o ƒë·ªÉ index")
            return

        start_time = time.time()
        print(f"B·∫Øt ƒë·∫ßu index {len(chunks)} chunks l√™n Qdrant cho user_id={user_id}")

        try:
            # T·∫°o embeddings batch ƒë·ªÉ t·ªëi ∆∞u t·ªëc ƒë·ªô
            texts = [chunk["text"] for chunk in chunks]
            print(f"ƒêang t·∫°o embeddings cho {len(texts)} texts...")

            # S·ª≠ d·ª•ng async batch encoding
            embeddings = await self.embedding_model.encode_batch(texts)
            print(f"ƒê√£ t·∫°o xong {len(embeddings)} embeddings")

            # L·∫•y file_id t·ª´ chunk ƒë·∫ßu ti√™n (gi·∫£ s·ª≠ t·∫•t c·∫£ chunks thu·ªôc c√πng m·ªôt file)
            file_id = chunks[0].get("file_id", str(uuid.uuid4()))

            # Index l√™n vector store
            await self.vector_store.index_documents(chunks, embeddings, user_id, file_id)

            end_time = time.time()
            processing_time = end_time - start_time
            print(
                f"Ho√†n th√†nh index {len(chunks)} chunks trong {processing_time:.2f}s "
                f"(trung b√¨nh {processing_time/len(chunks):.3f}s/chunk)"
            )

        except Exception as e:
            print(f"L·ªói khi index chunks: {str(e)}")
            raise

    def index_to_qdrant(self, chunks: List[Dict], user_id) -> None:
        """Index chunks l√™n Qdrant (ƒë·ªìng b·ªô)"""
        if not chunks:
            print("Kh√¥ng c√≥ chunks n√†o ƒë·ªÉ index")
            return

        start_time = time.time()
        print(f"B·∫Øt ƒë·∫ßu index {len(chunks)} chunks l√™n Qdrant cho user_id={user_id}")

        try:
            # T·∫°o embeddings batch ƒë·ªÉ t·ªëi ∆∞u t·ªëc ƒë·ªô
            texts = [chunk["text"] for chunk in chunks]
            print(f"ƒêang t·∫°o embeddings cho {len(texts)} texts...")

            # S·ª≠ d·ª•ng sync batch encoding
            embeddings = self.embedding_model.encode_batch_sync(texts)
            print(f"ƒê√£ t·∫°o xong {len(embeddings)} embeddings")

            # L·∫•y file_id t·ª´ chunk ƒë·∫ßu ti√™n (gi·∫£ s·ª≠ t·∫•t c·∫£ chunks thu·ªôc c√πng m·ªôt file)
            file_id = chunks[0].get("file_id", str(uuid.uuid4()))

            # Index l√™n vector store
            self.vector_store.index_documents_sync(chunks, embeddings, user_id, file_id)

            end_time = time.time()
            processing_time = end_time - start_time
            print(
                f"Ho√†n th√†nh index {len(chunks)} chunks trong {processing_time:.2f}s "
                f"(trung b√¨nh {processing_time/len(chunks):.3f}s/chunk)"
            )

        except Exception as e:
            print(f"L·ªói khi index chunks: {str(e)}")
            raise

    async def semantic_search_async(
        self,
        query: str,
        k: int = 20,
        sources: List[str] = None,
        file_id: List[str] = None,
    ) -> List[Dict]:
        """T√¨m ki·∫øm ng·ªØ nghƒ©a (b·∫•t ƒë·ªìng b·ªô)"""
        print(f"Semantic search v·ªõi query='{query}', k={k}")
        
        if sources:
            print(f"Filtering by sources: {sources}")
        if file_id:
            print(f"Filtering by file_id: {file_id}")

        try:
            # S·ª≠ d·ª•ng search manager v·ªõi async
            results = await self.search_manager.semantic_search(
                query=query,
                k=k,
                sources=sources,
                file_id=file_id,
            )

            print(f"T√¨m th·∫•y {len(results)} k·∫øt qu·∫£ t·ª´ semantic search")

            # Log th√¥ng tin k·∫øt qu·∫£ ƒë·ªÉ debug
            if results:
                for i, result in enumerate(results[:3]):  # Log 3 k·∫øt qu·∫£ ƒë·∫ßu
                    score = result.get("score", 0)
                    source = result.get("source", "unknown")
                    text_preview = result.get("text", "")[:100] + "..." if result.get("text", "") else ""
                    print(f"  Result {i+1}: score={score:.4f}, source={source}, text={text_preview}")

            return results

        except Exception as e:
            print(f"L·ªói trong semantic search: {str(e)}")
            import traceback
            print(f"Chi ti·∫øt l·ªói: {traceback.format_exc()}")
            return []

    def semantic_search(
        self,
        query: str,
        k: int = 20,
        sources: List[str] = None,
        file_id: List[str] = None,
    ) -> List[Dict]:
        """T√¨m ki·∫øm ng·ªØ nghƒ©a (ƒë·ªìng b·ªô)"""
        print(f"Semantic search v·ªõi query='{query}', k={k}")
        
        if sources:
            print(f"Filtering by sources: {sources}")
        if file_id:
            print(f"Filtering by file_id: {file_id}")

        try:
            # S·ª≠ d·ª•ng search manager v·ªõi sync
            results = self.search_manager.semantic_search_sync(
                query=query,
                k=k,
                sources=sources,
                file_id=file_id,
            )

            print(f"T√¨m th·∫•y {len(results)} k·∫øt qu·∫£ t·ª´ semantic search")

            # Log th√¥ng tin k·∫øt qu·∫£ ƒë·ªÉ debug
            if results:
                for i, result in enumerate(results[:3]):  # Log 3 k·∫øt qu·∫£ ƒë·∫ßu
                    score = result.get("score", 0)
                    source = result.get("source", "unknown")
                    text_preview = result.get("text", "")[:100] + "..." if result.get("text", "") else ""
                    print(f"  Result {i+1}: score={score:.4f}, source={source}, text={text_preview}")

            return results

        except Exception as e:
            print(f"L·ªói trong semantic search: {str(e)}")
            import traceback
            print(f"Chi ti·∫øt l·ªói: {traceback.format_exc()}")
            return []

    async def rerank_results_async(self, query: str, results: List[Dict]) -> List[Dict]:
        """T√°i x·∫øp h·∫°ng k·∫øt qu·∫£ (b·∫•t ƒë·ªìng b·ªô)"""
        return await self.search_manager.rerank_results(query, results)

    def rerank_results(self, query: str, results: List[Dict]) -> List[Dict]:
        """T√°i x·∫øp h·∫°ng k·∫øt qu·∫£ (ƒë·ªìng b·ªô)"""
        return self.search_manager.rerank_results_sync(query, results)

    async def query_with_sources_streaming(
        self,
        query: str,
        k: int = 20,
        sources: List[str] = None,
        file_id: List[str] = None,
        conversation_history: str = None,
    ) -> AsyncGenerator[Dict, None]:
        """
        Truy v·∫•n h·ªá th·ªëng RAG v·ªõi c√°c ngu·ªìn v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng stream

        Args:
            query: C√¢u h·ªèi ng∆∞·ªùi d√πng
            k: S·ªë l∆∞·ª£ng k·∫øt qu·∫£ tr·∫£ v·ªÅ
            sources: Danh s√°ch c√°c file ngu·ªìn c·∫ßn t√¨m ki·∫øm (c√°ch c≈©, s·ª≠ d·ª•ng file_id thay th·∫ø)
            file_id: Danh s√°ch c√°c file_id c·∫ßn t√¨m ki·∫øm (c√°ch m·ªõi). N·∫øu l√† None ho·∫∑c r·ªóng, s·∫Ω t√¨m ki·∫øm trong t·∫•t c·∫£ c√°c file
            conversation_history: L·ªãch s·ª≠ h·ªôi tho·∫°i

        Returns:
            AsyncGenerator tr·∫£ v·ªÅ t·ª´ng ph·∫ßn c·ªßa c√¢u tr·∫£ l·ªùi
        """
        print(f"ƒêang x·ª≠ l√Ω c√¢u h·ªèi (stream): '{query}'")
        
        if file_id is None or len(file_id) == 0:
            print("file_id l√† None ho·∫∑c danh s√°ch r·ªóng. T√¨m ki·∫øm s·∫Ω ƒë∆∞·ª£c th·ª±c hi·ªán tr√™n to√†n b·ªô t√†i li·ªáu.")
            # ƒê·∫∑t file_id th√†nh None ƒë·ªÉ semantic_search hi·ªÉu l√† t√¨m ki·∫øm tr√™n t·∫•t c·∫£ t√†i li·ªáu
            file_id = None
        else:
            print(f"T√¨m ki·∫øm v·ªõi file_id: {file_id}")

        # B·∫Øt ƒë·∫ßu ƒëo th·ªùi gian x·ª≠ l√Ω
        start_time = time.time()

        # X·ª≠ l√Ω v√† ph√¢n lo·∫°i c√¢u h·ªèi trong m·ªôt l·ªánh g·ªçi LLM duy nh·∫•t
        original_query = query
        expanded_query, query_type = await self.query_handler.expand_and_classify_query(
            query, conversation_history
        )
        query_to_use = expanded_query
        print(f"C√¢u h·ªèi ƒë√£ x·ª≠ l√Ω: '{query_to_use}', Lo·∫°i: '{query_type}'")

        # Tr·∫£ v·ªÅ ngay n·∫øu l√† c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn c∆° s·ªü d·ªØ li·ªáu
        if query_type == "other_question":
            # Tr·∫£ v·ªÅ th√¥ng b√°o b·∫Øt ƒë·∫ßu
            yield {
                "type": "start",
                "data": {
                    "query_type": query_type,
                    "file_id": file_id
                },
            }

            # Tr·∫£ v·ªÅ ngu·ªìn r·ªóng
            yield {
                "type": "sources",
                "data": {
                    "sources": [],
                    "filtered_sources": [],
                    "filtered_file_id": file_id if file_id else [],
                },
            }

            # Tr·∫£ v·ªÅ n·ªôi dung
            response = await self.query_handler.get_response_for_other_question(query)
            yield {"type": "content", "data": {"content": response}}

            # Tr·∫£ v·ªÅ k·∫øt th√∫c
            elapsed_time = time.time() - start_time
            yield {
                "type": "end",
                "data": {
                    "processing_time": round(elapsed_time, 2),
                    "query_type": query_type,
                },
            }
            return
        
        # X·ª≠ l√Ω sql_code_task tr·ª±c ti·∫øp v·ªõi LLM (streaming)
        elif query_type == "sql_code_task":
            print(f"C√¢u h·ªèi ƒë∆∞·ª£c ph√¢n lo·∫°i l√† sql_code_task (stream): '{query_to_use}'")
            yield {
                "type": "start",
                "data": {
                    "query": original_query,
                    "expanded_query": query_to_use if query_to_use != original_query else None,
                    "query_type": "sql_code_task",
                    "file_id": file_id,
                },
            }

            yield {
                "type": "sources",
                "data": {"sources": [], "filtered_file_id": file_id if file_id else []},
            }
            
            prompt_template = self.prompt_manager.templates.get("sql_code_task_prompt")
            if not prompt_template:
                yield {"type": "content", "data": {"content": "L·ªói: Kh√¥ng t√¨m th·∫•y prompt template cho SQL code task."}}
            else:
                # Chu·∫©n b·ªã ng·ªØ c·∫£nh h·ªôi tho·∫°i n·∫øu c√≥
                conversation_context_str = ""
                if conversation_history and conversation_history.strip():
                    conversation_context_str = f"NG·ªÆ C·∫¢NH CU·ªòC H·ªòI THO·∫†I:\n{conversation_history.strip()}\n"
                
                # Format prompt v·ªõi query v√† conversation context
                final_prompt = prompt_template.format(
                    query=query_to_use,
                    conversation_context=conversation_context_str
                )
                
                try:
                    async for content_chunk in self.llm.stream(final_prompt):
                        yield {"type": "content", "data": {"content": content_chunk}}
                except Exception as e:
                    print(f"L·ªói khi g·ªçi LLM stream cho sql_code_task: {str(e)}")
                    yield {
                        "type": "content",
                        "data": {
                            "content": f"Xin l·ªói, c√≥ l·ªói x·∫£y ra khi x·ª≠ l√Ω y√™u c·∫ßu SQL c·ªßa b·∫°n: {str(e)}"
                        },
                    }
            
            elapsed_time = time.time() - start_time
            yield {
                "type": "end",
                "data": {
                    "processing_time": round(elapsed_time, 2),
                    "query_type": "sql_code_task",
                },
            }
            return

        # Tr·∫£ v·ªÅ ngay n·∫øu l√† c√¢u h·ªèi th·ªùi gian th·ª±c
        if query_type == "realtime_question":
            # Tr·∫£ v·ªÅ th√¥ng b√°o b·∫Øt ƒë·∫ßu
            yield {
                "type": "start",
                "data": {
                    "query_type": query_type,
                    "search_type": "google_raw_search",
                    "file_id": file_id,
                },
            }

            # S·ª≠ d·ª•ng Google Search ƒë·ªÉ t√¨m ki·∫øm (k·∫øt qu·∫£ th√¥)
            try:
                # L·∫•y k·∫øt qu·∫£ th√¥ t·ª´ Google Search
                raw_search_content, gas_urls = get_raw_search_results(query_to_use)
                
                # Chu·∫©n b·ªã danh s√°ch ngu·ªìn t·ª´ Google Search
                gas_sources_list = []
                if gas_urls:
                    for url_idx, url in enumerate(gas_urls):
                        gas_sources_list.append({
                            "source": url,  # URL th·ª±c t·∫ø
                            "page": f"Web Search {url_idx+1}",
                            "section": "Online Source",
                            "score": 1.0,
                            "content_snippet": url,  # Hi·ªÉn th·ªã URL l√†m snippet
                            "file_id": "web_search",
                            "is_web_search": True,
                            "url": url  # Th√™m field url ri√™ng
                        })
                
                # Tr·∫£ v·ªÅ ngu·ªìn
                yield {
                    "type": "sources",
                    "data": {
                        "sources": gas_sources_list,
                        "filtered_sources": [],
                        "filtered_file_id": file_id if file_id else [],
                    },
                }

                # S·ª≠ d·ª•ng template REALTIME_QUESTION ƒë·ªÉ t·∫°o prompt
                prompt = self.prompt_manager.get_realtime_question_prompt(
                    query=query_to_use,
                    search_results=raw_search_content,
                    conversation_history=conversation_history
                )

                # G·ªçi LLM ƒë·ªÉ tr·∫£ l·ªùi d∆∞·ªõi d·∫°ng stream
                try:
                    async for content in self.llm.stream(prompt):
                        yield {"type": "content", "data": {"content": content}}
                except Exception as e:
                    print(f"L·ªói khi g·ªçi LLM stream cho realtime_question: {str(e)}")
                    # Tr·∫£ v·ªÅ l·ªói
                    yield {
                        "type": "content",
                        "data": {
                            "content": f"Xin l·ªói, c√≥ l·ªói x·∫£y ra khi x·ª≠ l√Ω c√¢u h·ªèi th·ªùi gian th·ª±c: {str(e)}"
                        },
                    }
            except Exception as e:
                print(f"L·ªói khi s·ª≠ d·ª•ng Google Search (stream): {str(e)}")
                # Tr·∫£ v·ªÅ ngu·ªìn r·ªóng
                yield {
                    "type": "sources",
                    "data": {
                        "sources": [],
                        "filtered_sources": [],
                        "filtered_file_id": file_id if file_id else [],
                    },
                }
                # Tr·∫£ v·ªÅ th√¥ng b√°o l·ªói
                yield {"type": "content", "data": {"content": f"Kh√¥ng th·ªÉ s·ª≠ d·ª•ng Google Search: {str(e)}. Vui l√≤ng ki·ªÉm tra l·∫°i API key ho·∫∑c c·∫•u h√¨nh."}}

            # Tr·∫£ v·ªÅ k·∫øt th√∫c
            elapsed_time = time.time() - start_time
            yield {
                "type": "end",
                "data": {
                    "processing_time": round(elapsed_time, 2),
                    "query_type": query_type,
                },
            }
            return
        RETRIEVAL_K = int(os.getenv("RETRIEVAL_K", "50"))
        # Th·ª±c hi·ªán semantic search
        search_results = await self.semantic_search_async(
            query_to_use, k=RETRIEVAL_K, sources=sources, file_id=file_id
        )
        
        # Fallback mechanism cho streaming
        perform_fallback_stream = not search_results or len(search_results) == 0
        gas_fallback_used = False
        
        if perform_fallback_stream:
            print(f"Kh√¥ng c√≥ k·∫øt qu·∫£ RAG (stream). Th·ª±c hi·ªán fallback v·ªõi Google Search cho: '{query_to_use}'")
            try:
                # Kh·ªüi t·∫°o sources_list tr∆∞·ªõc khi s·ª≠ d·ª•ng
                sources_list = []
                fallback_content, fallback_urls = get_raw_search_results(query_to_use)
                
                if fallback_content and fallback_content != "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan ƒë·∫øn truy v·∫•n n√†y.":
                    gas_fallback_used = True
                    print(f"ƒê√£ t√¨m th·∫•y k·∫øt qu·∫£ fallback: {fallback_content[:100]}...")
                    
                    # T·∫°o m·ªôt doc gi·∫£ cho fallback ƒë·ªÉ ƒë∆∞a v√†o context LLM
                    fallback_doc = {
                        "text": fallback_content,
                        "metadata": {
                            "source": "Google Search",
                            "page": "Web Result",
                            "source_type": "web_search",
                            "urls": fallback_urls
                        },
                        "score": 0.9,
                        "rerank_score": 0.9,
                        "file_id": "web_search_fallback"
                    }
                    
                    # Th√™m v√†o search_results
                    search_results = [fallback_doc]
                    
                    # C·∫≠p nh·∫≠t danh s√°ch ngu·ªìn v·ªõi k·∫øt qu·∫£ fallback
                    for url_idx, url in enumerate(fallback_urls):
                        sources_list.append({
                            "source": url,  # URL th·ª±c t·∫ø
                            "page": f"Fallback Search {url_idx+1}",
                            "section": "Online Source",
                            "score": 0.9,
                            "content_snippet": url,  # Hi·ªÉn th·ªã URL l√†m snippet
                            "file_id": "web_search_fallback",
                            "is_web_search": True,
                            "url": url  # Th√™m field url ri√™ng
                        })
                else:
                    print("Google Search kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ fallback.")
            except Exception as e:
                print(f"L·ªói khi th·ª±c hi·ªán fallback v·ªõi Google Search: {str(e)}")
                # Ti·∫øp t·ª•c v·ªõi k·∫øt qu·∫£ hi·ªán t·∫°i

        # N·∫øu kh√¥ng c√≥ k·∫øt qu·∫£ t√¨m ki·∫øm, tr·∫£ v·ªÅ th√¥ng b√°o kh√¥ng t√¨m th·∫•y
        if not search_results or len(search_results) == 0:
            # Tr·∫£ v·ªÅ th√¥ng b√°o b·∫Øt ƒë·∫ßu
            yield {
                "type": "start",
                "data": {
                    "query_type": "no_results",
                    "file_id": file_id,
                },
            }

            # Tr·∫£ v·ªÅ ngu·ªìn r·ªóng
            yield {
                "type": "sources",
                "data": {
                    "sources": [],
                    "filtered_sources": [],
                    "filtered_file_id": file_id if file_id else [],
                },
            }

            # Tr·∫£ v·ªÅ n·ªôi dung
            response = "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan ƒë·∫øn c√¢u h·ªèi c·ªßa b·∫°n trong t√†i li·ªáu. Vui l√≤ng th·ª≠ l·∫°i v·ªõi c√¢u h·ªèi kh√°c ho·∫∑c ƒëi·ªÅu ch·ªânh t·ª´ kh√≥a t√¨m ki·∫øm."
            yield {"type": "content", "data": {"content": response}}

            # Tr·∫£ v·ªÅ k·∫øt th√∫c
            elapsed_time = time.time() - start_time
            yield {
                "type": "end",
                "data": {
                    "processing_time": round(elapsed_time, 2),
                    "query_type": "no_results",
                },
            }
            return
        RERANK_TOP_N = int(os.getenv("RERANK_TOP_N", "15"))
        results_to_rerank = search_results[:RERANK_TOP_N]
        print(f"L·∫•y v·ªÅ {len(search_results)} k·∫øt qu·∫£, s·∫Ω rerank top {len(results_to_rerank)}.")

        # Rerank k·∫øt qu·∫£ n·∫øu c√≥ nhi·ªÅu h∆°n 1 k·∫øt qu·∫£
        if len(search_results) > 0:
            reranked_results = await self.rerank_results_async(
                query_to_use, results_to_rerank
            )
            # L·∫•y s·ªë l∆∞·ª£ng k·∫øt qu·∫£ ƒë√£ rerank
            total_reranked = len(reranked_results)
        else:
            reranked_results = search_results
            total_reranked = 1

        # Chu·∫©n b·ªã context t·ª´ c√°c k·∫øt qu·∫£ ƒë√£ rerank
        context_docs = []
        for i, result in enumerate(reranked_results[:20]):  
            # Chu·∫©n b·ªã metadata
            metadata = result.get("metadata", {})
            source = metadata.get("source", "unknown")
            
            # ∆Øu ti√™n s·ª≠ d·ª•ng page_label n·∫øu c√≥
            page = metadata.get("page_label", metadata.get("page", "N/A"))
            section = metadata.get("section", "N/A")

            # Th√™m v√†o danh s√°ch context
            context_docs.append(
                {
                    "content": result["text"],
                    "source": source,
                    "page": page,
                    "page_label": metadata.get("page_label", ""),  # Th√™m page_label v√†o context
                    "section": section,
                    "score": result.get("score", 0.0),
                    "metadata": metadata,  # Th√™m to√†n b·ªô metadata ƒë·ªÉ s·ª≠ d·ª•ng trong prompt
                }
            )

        # **TH√äM: Smart Fallback - ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng context**
        should_fallback = False
        fallback_reason = ""
        
        if self.enable_smart_fallback and context_docs and query_type == "question_from_document":
            print(f"üîç Evaluating context quality for smart fallback...")
            try:
                context_quality = await self._evaluate_context_quality(query_to_use, context_docs)
                
                # S·ª≠a logic: fallback khi confidence < threshold HO·∫∂C is_sufficient = False
                insufficient_context = not context_quality.get("is_sufficient", True)
                low_confidence = context_quality["confidence"] < self.context_quality_threshold
                
                if insufficient_context or low_confidence:
                    should_fallback = True
                    if insufficient_context:
                        fallback_reason = f"Insufficient context (is_sufficient=False, confidence: {context_quality['confidence']:.2f}, reason: {context_quality['reason']})"
                    else:
                        fallback_reason = f"Low context quality (confidence: {context_quality['confidence']:.2f} < threshold: {self.context_quality_threshold:.2f}, reason: {context_quality['reason']})"
                    print(f"üîÑ {fallback_reason}. Triggering smart fallback...")
                else:
                    print(f"‚úÖ Context quality sufficient (is_sufficient=True, confidence: {context_quality['confidence']:.2f} >= threshold: {self.context_quality_threshold:.2f}). Proceeding with RAG...")
            except Exception as e:
                print(f"‚ö†Ô∏è Error during context quality evaluation: {e}. Proceeding with normal RAG...")
        
        # **TH√äM: Execute smart fallback n·∫øu c·∫ßn**
        if should_fallback:
            print(f"üîÑ Executing smart fallback: {fallback_reason}")
            async for item in self._execute_google_fallback_streaming(
                query_to_use, conversation_history, start_time, query_type, file_id
            ):
                yield item
            return

        # Chu·∫©n b·ªã danh s√°ch ngu·ªìn tham kh·∫£o
        sources_list = []
        for i, doc in enumerate(reranked_results):
            # Tr√≠ch xu·∫•t th√¥ng tin t·ª´ metadata
            metadata = doc.get("metadata", {})
            
            # Ki·ªÉm tra n·∫øu l√† ngu·ªìn t·ª´ Google Search
            if metadata.get("source_type") == "web_search":
                urls_from_gas = metadata.get("urls", [])
                snippet = doc["text"]
                if urls_from_gas:
                    snippet += "\n\nNgu·ªìn tham kh·∫£o t·ª´ web:\n" + "\n".join([f"- {url}" for url in urls_from_gas])
                
                sources_list.append({
                    "source": "Google Search",
                    "page": "Web Search Result",
                    "section": "Web Content",
                    "score": doc.get("score", 0.9),
                    "content_snippet": snippet,
                    "file_id": doc.get("file_id", "web_search"),
                    "is_web_search": True
                })
            else:
                # Ngu·ªìn t·ª´ RAG th√¥ng th∆∞·ªùng
                source = metadata.get("source", "unknown")
                
                # ∆Øu ti√™n s·ª≠ d·ª•ng page_label n·∫øu c√≥
                page = metadata.get("page_label", metadata.get("page", "N/A"))
                page_label = metadata.get("page_label", "")
                section = metadata.get("section", "N/A")
                result_file_id = doc.get("file_id", "unknown")  # L·∫•y file_id t·ª´ k·∫øt qu·∫£

                # T·∫°o snippet t·ª´ n·ªôi dung
                content = doc["text"]
                snippet = content

                # Th√™m v√†o danh s√°ch ngu·ªìn
                sources_list.append(
                    {
                        "source": source,
                        "page": page,
                        "page_label": page_label,  # Th√™m page_label v√†o sources
                        "section": section,
                        "score": doc.get("score", 0.0),
                        "content_snippet": snippet,
                        "file_id": result_file_id,
                        "is_web_search": False,
                        "source_filename": os.path.basename(source) if os.path.sep in source else source,  # Th√™m t√™n file kh√¥ng c√≥ ƒë∆∞·ªùng d·∫´n
                    }
                )

        # Tr·∫£ v·ªÅ th√¥ng b√°o b·∫Øt ƒë·∫ßu
        yield {
            "type": "start",
            "data": {
                "query_type": query_type,
                "file_id": file_id,
                "total_results": len(search_results),
                "total_reranked": total_reranked,
            },
        }

        # Tr·∫£ v·ªÅ ngu·ªìn tham kh·∫£o
        yield {
            "type": "sources",
            "data": {
                "sources": sources_list,
                "filtered_sources": [],  # Gi·ªØ tr∆∞·ªùng n√†y ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi code c≈©
                "filtered_file_id": file_id if file_id else [],
            },
        }

        # Chu·∫©n b·ªã prompt cho LLM
        prompt = self.prompt_manager.create_prompt_with_history(
            query_to_use, context_docs, conversation_history=conversation_history
        )

        # G·ªçi LLM ƒë·ªÉ tr·∫£ l·ªùi v·ªõi response fallback detection
        try:
            # Thu th·∫≠p to√†n b·ªô response ƒë·ªÉ ki·ªÉm tra insufficient response patterns
            full_response = ""
            response_chunks = []
            
            # Stream response v√† l∆∞u l·∫°i t·ª´ng chunk
            async for content in self.llm.stream(prompt):
                full_response += content
                response_chunks.append(content)
                yield {"type": "content", "data": {"content": content}}
            
            # **TH√äM: Response-based Fallback Detection**
            # Ki·ªÉm tra xem response c√≥ ch·ª©a pattern thi·∫øu th√¥ng tin kh√¥ng
            if (self.enable_response_fallback and 
                query_type == "question_from_document" and 
                await self._detect_insufficient_response(full_response)):
                
                print(f"üîÑ Detected insufficient response, triggering Google search fallback...")
                
                # Yield m·ªôt d√≤ng tr·ªëng ƒë·ªÉ t√°ch bi·ªát v·ªõi response c≈©
                yield {"type": "content", "data": {"content": "\n\n---\n\n"}}
                yield {"type": "content", "data": {"content": "**ƒêang t√¨m ki·∫øm th√¥ng tin b·ªï sung t·ª´ web...**\n\n"}}
                
                # Execute Google Search fallback
                try:
                    async for fallback_item in self._execute_google_fallback_streaming(
                        query_to_use, conversation_history, start_time, f"{query_type}_response_fallback", file_id
                    ):
                        # Ch·ªâ yield content, kh√¥ng yield start/sources/end l·∫°i ƒë·ªÉ tr√°nh override
                        if fallback_item["type"] == "content":
                            yield fallback_item
                        elif fallback_item["type"] == "end":
                            # C·∫≠p nh·∫≠t query_type trong end message ƒë·ªÉ b√°o hi·ªáu ƒë√£ fallback
                            fallback_item["data"]["query_type"] = f"{query_type}_response_fallback"
                            yield fallback_item
                            return  # K·∫øt th√∫c lu√¥n sau fallback
                except Exception as fallback_error:
                    print(f"‚ùå Error in response-based fallback: {fallback_error}")
                    yield {"type": "content", "data": {"content": f"\n\nL·ªói khi t√¨m ki·∫øm th√¥ng tin b·ªï sung: {str(fallback_error)}"}}
                    
        except Exception as e:
            print(f"L·ªói khi g·ªçi LLM stream: {str(e)}")
            # Tr·∫£ v·ªÅ l·ªói
            yield {
                "type": "content",
                "data": {
                    "content": f"Xin l·ªói, c√≥ l·ªói x·∫£y ra khi x·ª≠ l√Ω c√¢u h·ªèi: {str(e)}"
                },
            }

        # K·∫øt th√∫c ƒëo th·ªùi gian
        elapsed_time = time.time() - start_time

        # Tr·∫£ v·ªÅ k·∫øt th√∫c
        yield {
            "type": "end",
            "data": {
                "processing_time": round(elapsed_time, 2),
                "query_type": query_type,
            },
        }

    def delete_collection(self) -> None:
        """X√≥a collection"""
        self.vector_store.delete_collection()

    def get_collection_info(self) -> Dict:
        """L·∫•y th√¥ng tin v·ªÅ collection"""
        return self.vector_store.get_collection_info()

    async def generate_related_questions(self, query: str, answer: str) -> List[str]:
        """T·∫°o danh s√°ch c√°c c√¢u h·ªèi g·ª£i √Ω li√™n quan s·ª≠ d·ª•ng SuggestionManager"""
        try:
            # T·∫°o conversation context t·ª´ Q&A hi·ªán t·∫°i
            conversation_context = f"Ng∆∞·ªùi d√πng: {query}\n\nTr·ª£ l√Ω: {answer}"
            
            # S·ª≠ d·ª•ng SuggestionManager thay v√¨ template
            suggestions = await self.suggestion_manager.generate_question_suggestions(
                conversation_context, num_suggestions=3
            )
            return suggestions[:3]  # ƒê·∫£m b·∫£o ch·ªâ tr·∫£ v·ªÅ 3 c√¢u h·ªèi
            
        except Exception as e:
            print(f"L·ªói khi t·∫°o c√¢u h·ªèi li√™n quan: {str(e)}")
            # Tr·∫£ v·ªÅ c√¢u h·ªèi m·∫∑c ƒë·ªãnh
            return [
                "B·∫°n mu·ªën t√¨m hi·ªÉu th√™m ƒëi·ªÅu g√¨ v·ªÅ ch·ªß ƒë·ªÅ n√†y?",
                "B·∫°n c√≥ th·∫Øc m·∫Øc n√†o kh√°c li√™n quan ƒë·∫øn n·ªôi dung n√†y kh√¥ng?",
                "B·∫°n c√≥ mu·ªën bi·∫øt th√™m th√¥ng tin v·ªÅ ·ª©ng d·ª•ng th·ª±c t·∫ø kh√¥ng?",
            ]

    async def _detect_insufficient_response(self, response_content: str) -> bool:
        """
        Ph√°t hi·ªán xem response c√≥ ch·ª©a pattern "kh√¥ng th·ªÉ tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß" kh√¥ng
        
        Args:
            response_content: N·ªôi dung response t·ª´ LLM
            
        Returns:
            bool: True n·∫øu response cho th·∫•y thi·∫øu th√¥ng tin
        """
        if not response_content or len(response_content.strip()) < 20:
            return False  # Response qu√° ng·∫Øn, kh√¥ng ƒë√°nh gi√°
        
        # C√°c pattern ch√≠nh x√°c cho th·∫•y thi·∫øu th√¥ng tin
        strong_insufficient_patterns = [
            "T√¥i kh√¥ng th·ªÉ tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß c√¢u h·ªèi n√†y d·ª±a tr√™n t√†i li·ªáu hi·ªán c√≥",
            "kh√¥ng th·ªÉ tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß c√¢u h·ªèi n√†y d·ª±a tr√™n t√†i li·ªáu",
            "kh√¥ng ƒë∆∞·ª£c t√¨m th·∫•y trong t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p",
            "T√¥i ch·ªâ t√¨m th·∫•y th√¥ng tin gi·ªõi h·∫°n v·ªÅ ch·ªß ƒë·ªÅ n√†y trong t√†i li·ªáu"
        ]
        
        # C√°c pattern y·∫øu h∆°n - c·∫ßn k·∫øt h·ª£p v·ªõi ƒëi·ªÅu ki·ªán kh√°c
        weak_insufficient_patterns = [
            "kh√¥ng ƒë·ªß th√¥ng tin",
            "th√¥ng tin kh√¥ng ƒë·∫ßy ƒë·ªß",
            "kh√¥ng c√≥ th√¥ng tin chi ti·∫øt",
            "t√†i li·ªáu kh√¥ng ƒë·ªÅ c·∫≠p",
            "kh√¥ng ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p trong t√†i li·ªáu"
        ]
        
        response_lower = response_content.lower()
        
        # Ki·ªÉm tra strong patterns - trigger fallback ngay l·∫≠p t·ª©c
        for pattern in strong_insufficient_patterns:
            if pattern.lower() in response_lower:
                print(f"üîç Detected strong insufficient response pattern: '{pattern}' in response")
                return True
        
        # Ki·ªÉm tra weak patterns - ch·ªâ trigger n·∫øu response ng·∫Øn
        response_length = len(response_content.strip())
        if response_length < 200:  # Response ng·∫Øn h∆°n 200 k√Ω t·ª±
            for pattern in weak_insufficient_patterns:
                if pattern.lower() in response_lower:
                    print(f"üîç Detected weak insufficient response pattern: '{pattern}' in short response ({response_length} chars)")
                    return True
        
        # Ki·ªÉm tra xem c√≥ ph·∫£i ch·ªâ l√† c√¢u tr·∫£ l·ªùi t·ª´ ch·ªëi hay kh√¥ng
        refusal_indicators = [
            "t√¥i kh√¥ng th·ªÉ",
            "kh√¥ng th·ªÉ cung c·∫•p",
            "kh√¥ng c√≥ th√¥ng tin",
            "xin l·ªói, t√¥i"
        ]
        
        # N·∫øu response ch·ª©a nhi·ªÅu t·ª´ t·ª´ ch·ªëi v√† ng·∫Øn -> c√≥ th·ªÉ thi·∫øu th√¥ng tin
        refusal_count = sum(1 for indicator in refusal_indicators if indicator in response_lower)
        if refusal_count >= 2 and response_length < 300:
            print(f"üîç Detected multiple refusal indicators ({refusal_count}) in short response ({response_length} chars)")
            return True
        
        return False

