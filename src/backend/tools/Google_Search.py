import os
import logging
import asyncio
import time
import hashlib
from typing import Tuple, List, Any, Optional, Dict
from dataclasses import dataclass
from functools import lru_cache
from datetime import datetime, timedelta
import json

from langchain.agents import initialize_agent, Tool
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.tools import TavilySearchResults
from dotenv import load_dotenv

load_dotenv()

# C·∫•u h√¨nh logging v·ªõi format t√πy ch·ªânh
logging.basicConfig(
    level=logging.INFO, 
    format='[Google_Search] %(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class SearchConfig:
    """C·∫•u h√¨nh cho Google Search tool"""
    max_results: int = 2
    include_answer: bool = True
    include_raw_content: bool = True
    temperature: float = 0.0
    model: str = "gemini-1.5-flash"
    cache_ttl_hours: int = 24
    rate_limit_per_minute: int = 30
    max_content_length: int = 8000
    timeout_seconds: int = 30
    enable_llm: bool = False

class RateLimiter:
    """Simple rate limiter ƒë·ªÉ tr√°nh spam API"""
    
    def __init__(self, max_calls_per_minute: int = 30):
        self.max_calls = max_calls_per_minute
        self.calls = []
    
    def can_make_call(self) -> bool:
        """Ki·ªÉm tra xem c√≥ th·ªÉ g·ªçi API kh√¥ng"""
        now = datetime.now()
        # Lo·∫°i b·ªè c√°c calls c≈© h∆°n 1 ph√∫t
        self.calls = [call_time for call_time in self.calls 
                     if now - call_time < timedelta(minutes=1)]
        
        if len(self.calls) < self.max_calls:
            self.calls.append(now)
            return True
        return False
    
    def wait_time(self) -> float:
        """T√≠nh th·ªùi gian c·∫ßn ƒë·ª£i (seconds)"""
        if not self.calls:
            return 0.0
        oldest_call = min(self.calls)
        wait_until = oldest_call + timedelta(minutes=1)
        wait_seconds = (wait_until - datetime.now()).total_seconds()
        return max(0.0, wait_seconds)

class SearchCache:
    """Cache system cho search results"""
    
    def __init__(self, ttl_hours: int = 24):
        self.cache: Dict[str, Dict] = {}
        self.ttl_hours = ttl_hours
    
    def _get_cache_key(self, query: str) -> str:
        """T·∫°o cache key t·ª´ query"""
        return hashlib.md5(query.lower().strip().encode()).hexdigest()
    
    def get(self, query: str) -> Optional[Tuple[Any, List[str]]]:
        """L·∫•y k·∫øt qu·∫£ t·ª´ cache"""
        key = self._get_cache_key(query)
        if key not in self.cache:
            return None
        
        entry = self.cache[key]
        # Ki·ªÉm tra TTL
        cache_time = datetime.fromisoformat(entry['timestamp'])
        if datetime.now() - cache_time > timedelta(hours=self.ttl_hours):
            del self.cache[key]
            return None
        
        logger.info(f"üéØ Cache hit cho query: {query[:50]}...")
        return entry['result'], entry['sources']
    
    def set(self, query: str, result: Any, sources: List[str]):
        """L∆∞u k·∫øt qu·∫£ v√†o cache"""
        key = self._get_cache_key(query)
        self.cache[key] = {
            'result': result,
            'sources': sources,
            'timestamp': datetime.now().isoformat()
        }
        logger.info(f"üíæ Cached result cho query: {query[:50]}...")
    
    def clear_expired(self):
        """X√≥a c√°c entry ƒë√£ h·∫øt h·∫°n"""
        now = datetime.now()
        expired_keys = []
        
        for key, entry in self.cache.items():
            cache_time = datetime.fromisoformat(entry['timestamp'])
            if now - cache_time > timedelta(hours=self.ttl_hours):
                expired_keys.append(key)
        
        for key in expired_keys:
            del self.cache[key]
        
        if expired_keys:
            logger.info(f"üóëÔ∏è ƒê√£ x√≥a {len(expired_keys)} cache entries h·∫øt h·∫°n")

class OptimizedGoogleSearch:
    """
    Google Search tool ƒë∆∞·ª£c t·ªëi ∆∞u v·ªõi caching, rate limiting v√† error handling n√¢ng cao
    """
    
    def __init__(self, config: SearchConfig = SearchConfig()):
        self.config = config
        self.cache = SearchCache(config.cache_ttl_hours)
        self.rate_limiter = RateLimiter(config.rate_limit_per_minute)
        
        # Kh·ªüi t·∫°o API keys
        self.google_api_key = os.getenv('API_KEY_LLM_SEARCH_TOOL')
        self.tavily_api_key = os.getenv('TAVILY_API_KEY')
        
        # Validate API keys
        self._validate_api_keys()
        
        # Kh·ªüi t·∫°o components
        self.llm = None
        self.tavily_search = None
        self._initialize_components()
    
    def _validate_api_keys(self):
        """Validate API keys"""
        if not self.google_api_key:
            logger.error("‚ùå GOOGLE_API_KEY kh√¥ng ƒë∆∞·ª£c t√¨m th·∫•y trong bi·∫øn m√¥i tr∆∞·ªùng")
            raise ValueError("Missing Google API key")
        
        if not self.tavily_api_key:
            logger.error("‚ùå TAVILY_API_KEY kh√¥ng ƒë∆∞·ª£c t√¨m th·∫•y trong bi·∫øn m√¥i tr∆∞·ªùng")
            raise ValueError("Missing Tavily API key")
        
        # Set environment variables
        os.environ["GEMINI_API_KEY"] = self.google_api_key
        os.environ["TAVILY_API_KEY"] = self.tavily_api_key
        
        logger.info("‚úÖ API keys validated successfully")
    
    def _initialize_components(self):
        """Kh·ªüi t·∫°o LLM v√† search components"""
        try:
            self.llm = ChatGoogleGenerativeAI(
                model=self.config.model,
                temperature=self.config.temperature,
                google_api_key=self.google_api_key,
                timeout=self.config.timeout_seconds
            )
            
            self.tavily_search = TavilySearchResults(
                max_results=self.config.max_results,
                include_answer=self.config.include_answer,
                include_raw_content=self.config.include_raw_content
            )
            
            logger.info("‚úÖ Search components initialized successfully")
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói kh·ªüi t·∫°o components: {str(e)}")
            raise
    
    def _validate_query(self, query: str) -> str:
        """Validate v√† clean query"""
        if not query or not query.strip():
            raise ValueError("Query kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng")
        
        query = query.strip()
        if len(query) > 500:
            logger.warning(f"‚ö†Ô∏è Query qu√° d√†i ({len(query)} chars), s·∫Ω truncate")
            query = query[:500] + "..."
        
        return query
    
    def _process_tavily_results(self, results: Any) -> Tuple[List[str], str]:
        """X·ª≠ l√Ω k·∫øt qu·∫£ t·ª´ Tavily v·ªõi validation"""
        urls = []
        contents = []
        
        try:
            if isinstance(results, list):
                for item in results:
                    if isinstance(item, dict) and "url" in item:
                        urls.append(item["url"])
                        title = item.get('title', 'No title')
                        content = item.get('content', 'No content')
                        
                        # Truncate content n·∫øu qu√° d√†i
                        if len(content) > self.config.max_content_length:
                            content = content[:self.config.max_content_length] + "..."
                        
                        contents.append(f"- **{title}** ({item['url']})\n{content}\n")
                        
            elif isinstance(results, dict) and "url" in results:
                urls.append(results["url"])
                title = results.get('title', 'No title')
                content = results.get('content', 'No content')
                
                if len(content) > self.config.max_content_length:
                    content = content[:self.config.max_content_length] + "..."
                
                contents.append(f"- **{title}** ({results['url']})\n{content}\n")
            
            return urls, "\n".join(contents)
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói x·ª≠ l√Ω Tavily results: {str(e)}")
            return [], ""
    
    def _create_optimized_prompt(self, query: str, content: str) -> str:
        """T·∫°o prompt ƒë∆∞·ª£c t·ªëi ∆∞u cho LLM"""
        return f"""B·∫°n l√† chuy√™n gia c∆° s·ªü d·ªØ li·ªáu. H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n k·∫øt qu·∫£ t√¨m ki·∫øm.

**NGUY√äN T·∫ÆC QUAN TR·ªåNG:**
‚úÖ **B·∫ÆT BU·ªòC:** Cu·ªëi c√¢u tr·∫£ l·ªùi PH·∫¢I c√≥ ph·∫ßn "## Ngu·ªìn tham kh·∫£o" v·ªõi t·∫•t c·∫£ URL ƒë∆∞·ª£c t√¨m th·∫•y
‚úÖ **ƒê·ªäNH D·∫†NG:** S·ª≠ d·ª•ng Markdown chu·∫©n (##, ###, **, -, ```sql)  
‚úÖ **N·ªòI DUNG:** Tr·∫£ l·ªùi ch√≠nh x√°c, ng·∫Øn g·ªçn, d·ªÖ hi·ªÉu v·ªÅ c∆° s·ªü d·ªØ li·ªáu
‚úÖ **NG√îN NG·ªÆ:** Ti·∫øng Vi·ªát, gi·ªØ nguy√™n thu·∫≠t ng·ªØ chuy√™n ng√†nh

**V√ç D·ª§ ƒê·ªäNH D·∫†NG:**
C√¢u tr·∫£ l·ªùi ch√≠nh v·ªÅ ch·ªß ƒë·ªÅ...

## C√°c lo·∫°i CSDL ph·ªï bi·∫øn
- **PostgreSQL**: M√£ ngu·ªìn m·ªü, m·∫°nh m·∫Ω
- **MySQL**: Ph·ªï bi·∫øn cho web
- **SQL Server**: C·ªßa Microsoft

## Ngu·ªìn tham kh·∫£o  
- https://example1.com
- https://example2.com

**K·∫æT QU·∫¢ T√åM KI·∫æM:**
{content}

**C√ÇU H·ªéI:** {query}

H√£y tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin tr√™n v√† NH·∫§T ƒê·ªäNH ph·∫£i c√≥ ph·∫ßn "## Ngu·ªìn tham kh·∫£o" ·ªü cu·ªëi v·ªõi t·∫•t c·∫£ URL."""
    
    def _validate_llm_response(self, response: Any) -> str:
        """Validate v√† clean LLM response"""
        try:
            if hasattr(response, 'content'):
                content = response.content
            elif isinstance(response, str):
                content = response
            else:
                content = str(response)
            
            # Ki·ªÉm tra xem c√≥ URL ngu·ªìn kh√¥ng
            if '[http' not in content and '[www' not in content:
                logger.warning("‚ö†Ô∏è LLM response thi·∫øu URL ngu·ªìn")
            
            return content.strip()
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói validate LLM response: {str(e)}")
            return "L·ªói x·ª≠ l√Ω ph·∫£n h·ªìi t·ª´ AI."
    
    def search_raw_results(self, query: str) -> Tuple[str, List[str]]:
        """
        T√¨m ki·∫øm v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ th√¥ (kh√¥ng qua LLM processing)
        
        Args:
            query: C√¢u h·ªèi c·∫ßn t√¨m ki·∫øm
            
        Returns:
            Tuple[str, List[str]]: (raw_search_content, source_urls)
        """
        try:
            # Validate query
            query = self._validate_query(query)
            
            # Check cache tr∆∞·ªõc
            cached_result = self.cache.get(query)
            if cached_result:
                return cached_result
            
            # Rate limiting
            if not self.rate_limiter.can_make_call():
                wait_time = self.rate_limiter.wait_time()
                logger.warning(f"‚è≥ Rate limit reached. ƒê·ª£i {wait_time:.1f}s...")
                time.sleep(wait_time)
            
            # Validate components
            if not self.llm or not self.tavily_search:
                error_msg = "‚ùå Search components ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o"
                logger.error(error_msg)
                return error_msg, []
            
            logger.info(f"üîç ƒêang t√¨m ki·∫øm: {query[:100]}...")
            
            # G·ªçi Tavily API
            start_time = time.time()
            results = self.tavily_search.invoke({"query": query})
            search_time = time.time() - start_time
            
            logger.info(f"üìä Tavily search completed in {search_time:.2f}s")
            
            # X·ª≠ l√Ω k·∫øt qu·∫£
            urls, content = self._process_tavily_results(results)
            
            if not content:
                no_result_msg = "üîç Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan ƒë·∫øn truy v·∫•n n√†y."
                logger.warning(no_result_msg)
                return no_result_msg, []
            
            # Tr·∫£ v·ªÅ k·∫øt qu·∫£ th√¥ (kh√¥ng qua LLM processing)
            # Cache k·∫øt qu·∫£ th√¥
            self.cache.set(query, content, urls)
            
            total_time = time.time() - start_time
            logger.info(f"‚úÖ Raw search ho√†n th√†nh trong {total_time:.2f}s v·ªõi {len(urls)} ngu·ªìn")
            
            return content, urls
            
        except Exception as e:
            error_msg = f"‚ùå L·ªói t√¨m ki·∫øm: {str(e)}"
            logger.error(error_msg)
            return error_msg, []

    def search_with_sources(self, query: str) -> Tuple[str, List[str]]:
        """
        T√¨m ki·∫øm v·ªõi sources v√† LLM processing (ƒë·ªÉ backward compatibility)
        
        Args:
            query: C√¢u h·ªèi c·∫ßn t√¨m ki·∫øm
            
        Returns:
            Tuple[str, List[str]]: (processed_answer, source_urls)
        """
        try:
            # L·∫•y k·∫øt qu·∫£ th√¥
            raw_content, urls = self.search_raw_results(query)
            
            if not raw_content or raw_content.startswith("üîç Kh√¥ng t√¨m th·∫•y"):
                return raw_content, urls
            
            # X·ª≠ l√Ω v·ªõi LLM n·∫øu c·∫ßn
            prompt = self._create_optimized_prompt(query, raw_content)
            
            logger.info("ü§ñ ƒêang x·ª≠ l√Ω v·ªõi LLM...")
            llm_start = time.time()
            
            try:
                llm_response = self.llm.invoke(prompt)
                llm_time = time.time() - llm_start
                logger.info(f"üß† LLM processing completed in {llm_time:.2f}s")
                
                # Validate v√† clean response
                final_response = self._validate_llm_response(llm_response)
                return final_response, urls
                
            except Exception as llm_error:
                logger.error(f"‚ùå LLM error: {str(llm_error)}")
                fallback_msg = f"L·ªói x·ª≠ l√Ω AI, nh∆∞ng ƒë√£ t√¨m th·∫•y {len(urls)} ngu·ªìn li√™n quan."
                return fallback_msg, urls
                
        except Exception as e:
            error_msg = f"‚ùå L·ªói t√¨m ki·∫øm: {str(e)}"
            logger.error(error_msg)
            return error_msg, []
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """L·∫•y th·ªëng k√™ cache"""
        return {
            "cache_size": len(self.cache.cache),
            "rate_limit_calls": len(self.rate_limiter.calls),
            "max_calls_per_minute": self.rate_limiter.max_calls
        }
    
    def clear_cache(self):
        """X√≥a cache"""
        self.cache.cache.clear()
        logger.info("üóëÔ∏è Cache ƒë√£ ƒë∆∞·ª£c x√≥a")
    
    def cleanup_expired_cache(self):
        """Cleanup expired cache entries"""
        self.cache.clear_expired()

# Global instance
_search_instance = None

def get_search_instance(config: SearchConfig = SearchConfig()) -> OptimizedGoogleSearch:
    """Singleton pattern ƒë·ªÉ t√°i s·ª≠ d·ª•ng instance"""
    global _search_instance
    if _search_instance is None:
        _search_instance = OptimizedGoogleSearch(config)
    return _search_instance

# Backwards compatibility functions
def tavily_with_sources(query: str) -> Tuple[List[str], str]:
    """Legacy function for backwards compatibility"""
    logger.warning("‚ö†Ô∏è ƒêang s·ª≠ d·ª•ng legacy function. Khuy·∫øn ngh·ªã d√πng OptimizedGoogleSearch class")
    search = get_search_instance()
    response, urls = search.search_with_sources(query)
    return urls, response

def run_query_with_sources(query: str) -> Tuple[Any, List[str]]:
    """Legacy function for backwards compatibility"""
    logger.warning("‚ö†Ô∏è ƒêang s·ª≠ d·ª•ng legacy function. Khuy·∫øn ngh·ªã d√πng OptimizedGoogleSearch class")
    search = get_search_instance()
    return search.search_with_sources(query)

def get_raw_search_results(query: str) -> Tuple[str, List[str]]:
    """Function to get raw search results without LLM processing"""
    search = get_search_instance()
    return search.search_raw_results(query)

# Main execution
if __name__ == "__main__":
    # Test v·ªõi multiple queries
    test_queries = [
        "C√∫ ph√°p select m·ªõi nh·∫•t v√† ƒë·∫ßy ƒë·ªß c·ªßa l·ªánh select trong csdl?",
        "Best practices for database indexing in 2024",
        "Kh√°i ni·ªám ACID trong c∆° s·ªü d·ªØ li·ªáu"
    ]
    
    config = SearchConfig(
        max_results=3,
        cache_ttl_hours=1,  # Test cache
        rate_limit_per_minute=20
    )
    
    search = OptimizedGoogleSearch(config)
    
    for query in test_queries:
        print(f"\n{'='*60}")
        print(f"üîç Query: {query}")
        print('='*60)
        
        start_time = time.time()
        response, sources = search.search_with_sources(query)
        elapsed = time.time() - start_time
        
        print(f"\n‚è±Ô∏è Th·ªùi gian: {elapsed:.2f}s")
        print(f"üìä S·ªë ngu·ªìn: {len(sources)}")
        print(f"\nüìù K·∫øt qu·∫£:\n{response}")
        print(f"\nüîó Ngu·ªìn tham kh·∫£o:")
        for i, url in enumerate(sources, 1):
            print(f"  {i}. {url}")
        
        # Test cache l·∫ßn 2
        print(f"\nüîÑ Testing cache...")
        start_time = time.time()
        cached_response, cached_sources = search.search_with_sources(query)
        cache_elapsed = time.time() - start_time
        print(f"‚ö° Cache hit time: {cache_elapsed:.3f}s")
    
    # Print cache stats
    stats = search.get_cache_stats()
    print(f"\nüìä Cache Stats: {stats}")
