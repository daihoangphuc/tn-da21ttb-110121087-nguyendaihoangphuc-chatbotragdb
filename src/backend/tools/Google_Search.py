"""
optimized_google_search.py
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Module t√¨m ki·∫øm (Google + Tavily) t·ªëi ∆∞u cho h·ªá th·ªëng RAG.

‚Ä¢ N·∫øu c√≥ SERPER_API_KEY  ‚Üí ∆∞u ti√™n Google (Serper.dev) v√† l·∫•y full‚Äëtext c·ªßa
  N URL ƒë·∫ßu ti√™n (google_top_k).
‚Ä¢ Sau ƒë√≥ m·ªõi fallback Tavily basic / advanced.
‚Ä¢ Tr·∫£ v·ªÅ tuple (markdown_content, url_list) ‚Äì¬†gi·ªØ nguy√™n ch·ªØ k√Ω h√†m c≈©.

CH·ªÆ K√ù PUBLIC (KH√îNG THAY ƒê·ªîI)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
get_search_instance()
run_query_with_sources(query)
get_raw_search_results(query)
tavily_with_sources(query)

ENV b·∫Øt bu·ªôc:
‚Ä¢ TAVILY_API_KEY
TuÃÄy ch·ªçn:
‚Ä¢ SERPER_API_KEY          ‚Äì k√≠ch ho·∫°t Serper (Google)
‚Ä¢ GOOGLE_API_KEY|GEMINI_API_KEY ‚Äì k√≠ch ho·∫°t Gemini t√≥m t·∫Øt
"""

from __future__ import annotations

import hashlib
import html2text
import logging
import os
import re
import time
import unicodedata
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple
from langchain_tavily import TavilySearch
import requests
from readability import Document  # pip install readability-lxml
# LangChain tools -----------------------------------------------------------
from langchain_community.tools import TavilySearchResults  # type: ignore
try:
    from langchain_community.tools import GoogleSerperResults  # type: ignore
except ImportError:                                            # pragma: no cover
    GoogleSerperResults = None

# Gemini LLM (tuÃÄy ch·ªçn) -----------------------------------------------------
try:
    from langchain_google_genai import ChatGoogleGenerativeAI  # type: ignore
except ImportError:                                            # pragma: no cover
    ChatGoogleGenerativeAI = None  # kh√¥ng b·∫Øt bu·ªôc

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
logging.basicConfig(
    level=logging.INFO,
    format="[Google_Search] %(asctime)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


# ============================= CONFIG ======================================

@dataclass
class SearchConfig:
    # H√†nh vi chung
    max_results: int = 3
    min_docs: int = 3
    max_content_length: int = 4000
    cache_ttl_hours: int = 24
    rate_limit_per_minute: int = 30

    # Google ∆∞u ti√™n
    prefer_google: bool = True
    google_top_k: int = 3   # s·ªë URL ƒë·∫ßu ti√™n s·∫Ω crawl n·ªôi dung

    # Tavily
    include_answer: bool = True
    include_raw_content: bool = False
    search_depth_basic: str = "basic"
    search_depth_advanced: str = "advanced"
    escalate_to_advanced: bool = True

    # Serper (Google)
    use_serper: bool = True
    serper_max_results: int = 10

    # LLM (tuÃÄy ch·ªçn)
    enable_llm: bool = False
    llm_temperature: float = 0.0
    llm_model: str = "gemini-1.5-flash"


# ============================= HELPERS =====================================

class RateLimiter:
    """Gi·ªõi h·∫°n s·ªë l·∫ßn g·ªçi API / ph√∫t (c·ª±c ƒë∆°n gi·∫£n)."""

    def __init__(self, max_calls_per_minute: int):
        self.max_calls = max_calls_per_minute
        self.calls: List[datetime] = []

    def can_make_call(self) -> bool:
        now = datetime.now()
        self.calls = [t for t in self.calls if now - t < timedelta(minutes=1)]
        if len(self.calls) < self.max_calls:
            self.calls.append(now)
            return True
        return False

    def wait_time(self) -> float:
        if not self.calls:
            return 0.0
        oldest = min(self.calls)
        return max(0.0, (oldest + timedelta(minutes=1) - datetime.now()).total_seconds())


class SearchCache:
    """Cache trong RAM ‚Äì¬†ƒë·ªß nhanh cho h·∫ßu h·∫øt use‚Äëcase."""

    def __init__(self, ttl_hours: int):
        self.ttl = ttl_hours
        self.store: Dict[str, Dict] = {}

    def _key(self, query: str) -> str:
        return hashlib.md5(query.lower().strip().encode()).hexdigest()

    def get(self, query: str) -> Optional[Tuple[str, List[str]]]:
        k = self._key(query)
        entry = self.store.get(k)
        if not entry:
            return None
        if datetime.now() - entry["ts"] > timedelta(hours=self.ttl):
            del self.store[k]
            return None
        logger.info("üéØ Cache hit cho query: %s‚Ä¶", query[:60])
        return entry["content"], entry["urls"]

    def set(self, query: str, content: str, urls: List[str]):
        self.store[self._key(query)] = {
            "content": content,
            "urls": urls,
            "ts": datetime.now(),
        }


# ========================== MAIN CLASS =====================================

class OptimizedGoogleSearch:
    """Serper (Google) ‚ûú Tavily basic ‚ûú Tavily advanced (fallback)."""

    # ---------------------------------------------------------------------

    def __init__(self, config: SearchConfig = SearchConfig()):
        self.cfg = config
        self.rate_limiter = RateLimiter(config.rate_limit_per_minute)
        self.cache = SearchCache(config.cache_ttl_hours)

        if not os.getenv("TAVILY_API_KEY"):
            raise ValueError("‚ùå Thi·∫øu TAVILY_API_KEY trong .env")

        self._init_search_tools()
        self._init_llm()

    # ---------------------------------------------------------------------

    def _init_search_tools(self):
        # Serper (Google)
        self.serper: Optional[GoogleSerperResults] = None
        if (
            self.cfg.use_serper
            and os.getenv("SERPER_API_KEY")
            and GoogleSerperResults is not None
        ):
            self.serper = GoogleSerperResults(k=self.cfg.serper_max_results)
            logger.info("‚úÖ Serper (Google) initialized")
        else:
            logger.info("‚ÑπÔ∏è Serper disabled")

        # Tavily
        common = dict(
            include_answer=self.cfg.include_answer,
            include_raw_content=self.cfg.include_raw_content,
        )
        self.tavily_basic = TavilySearchResults(
            max_results=self.cfg.max_results,
            search_depth=self.cfg.search_depth_basic,
            **common,
        )
        self.tavily_advanced = TavilySearchResults(
            max_results=max(self.cfg.max_results, 10),
            search_depth=self.cfg.search_depth_advanced,
            **common,
        )

    def _init_llm(self):
        self.llm = None
        google_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
        if self.cfg.enable_llm and google_key and ChatGoogleGenerativeAI:
            os.environ["GEMINI_API_KEY"] = google_key
            self.llm = ChatGoogleGenerativeAI(
                model=self.cfg.llm_model,
                temperature=self.cfg.llm_temperature,
            )
            logger.info("‚úÖ Gemini LLM ready")

    # ===================== GOOGLE HELPERS =================================

    @staticmethod
    def _process_serper_results(res: Any, max_len: int) -> Tuple[List[str], str]:
        """Tr·∫£ v·ªÅ (urls, markdown snippet)."""
        urls, md = [], []
        if not isinstance(res, dict):
            return urls, ""
        # Serper gi·ªØ th·ª© t·ª± organic theo 'position'
        for item in res.get("organic", []):
            url = item.get("link")
            if not url or url in urls:
                continue
            urls.append(url)
            title = item.get("title", "No title")
            snippet = item.get("snippet", "")[:max_len]
            md.append(f"- **{title}** ({url})\n{snippet}\n")
        return urls, "\n".join(md)

    @staticmethod
    def _process_tavily_results(res: Any, max_len: int) -> Tuple[List[str], str]:
        urls, md = [], []
        iterable = res if isinstance(res, list) else [res]
        for item in iterable:
            if not isinstance(item, dict) or "url" not in item:
                continue
            url = item["url"]
            if url in urls:
                continue
            urls.append(url)
            title = item.get("title", "No title")
            content = item.get("content", "")[:max_len]
            if len(item.get("content", "")) > max_len:
                content += "‚Ä¶"
            md.append(f"- **{title}** ({url})\n{content}\n")
        return urls, "\n".join(md)

    # ---------- Crawl page text ------------------------------------------

    @staticmethod
    def _fetch_page_content(url: str, max_len: int = 2_000) -> str:
        try:
            resp = requests.get(
                url,
                timeout=8,
                headers={
                    "User-Agent": "Mozilla/5.0 (RAG/1.0) AppleWebKit/537.36"
                },
            )
            resp.raise_for_status()
            doc = Document(resp.text)
            text = html2text.html2text(doc.summary())
            text = unicodedata.normalize("NFKC", re.sub(r"\s+", " ", text))
            return text[:max_len] + ("‚Ä¶" if len(text) > max_len else "")
        except Exception:
            logger.debug("‚ö†Ô∏è Kh√¥ng l·∫•y ƒë∆∞·ª£c n·ªôi dung %s", url)
            return ""

    # =================== PUBLIC METHODS ===================================

    def search_raw_results(self, query: str) -> Tuple[str, List[str]]:
        """Tr·∫£ v·ªÅ (markdown, urls)."""
        query = query.strip()
        if not query:
            raise ValueError("Query tr·ªëng")

        # Cache
        hit = self.cache.get(query)
        if hit:
            return hit

        # Rate limit
        if not self.rate_limiter.can_make_call():
            time.sleep(self.rate_limiter.wait_time())

        urls: List[str] = []
        content_parts: List[str] = []

        # 1Ô∏è‚É£ SERPER (Google)
        if self.serper:
            try:
                s_res = self.serper.invoke({"query": query})
                urls_s, md_s = self._process_serper_results(
                    s_res, self.cfg.max_content_length
                )
                # Crawl n·ªôi dung Top‚ÄëK n·∫øu prefer_google
                if self.cfg.prefer_google:
                    top_urls = urls_s[: self.cfg.google_top_k]
                    if top_urls:                       # <‚Äî th√™m
                        with ThreadPoolExecutor(max_workers=min(len(top_urls), 8)) as ex:
                            futures = {
                                ex.submit(
                                    self._fetch_page_content, u, self.cfg.max_content_length
                                ): u
                                for u in top_urls
                            }
                        for fut in as_completed(futures):
                            u = futures[fut]
                            txt = fut.result()
                            if txt:
                                texts[u] = txt
                    for u in top_urls:
                        if u in texts:
                            content_parts.append(f"### {u}\n{texts[u]}\n")
                        if u not in urls:
                            urls.append(u)

                    # N·∫øu c√≤n URL Google ngo√†i Top‚ÄëK ‚Üí th√™m cu·ªëi danh s√°ch
                    for u in urls_s[self.cfg.google_top_k :]:
                        if u not in urls:
                            urls.append(u)
                    # snippets Google (md_s) v·∫´n h·ªØu √≠ch ƒë·ªÉ LLM hi·ªÉu ng·ªØ c·∫£nh
                    content_parts.append(md_s)
                else:
                    urls.extend(urls_s)
                    content_parts.append(md_s)
            except Exception as e:  # pragma: no cover
                logger.warning("Serper error: %s ‚Äì b·ªè qua", e)

        # 2Ô∏è‚É£ TAVILY BASIC
        try:
            t_basic = self.tavily_basic.invoke({"query": query})
            urls_t, md_t = self._process_tavily_results(
                t_basic, self.cfg.max_content_length
            )
            urls.extend([u for u in urls_t if u not in urls])
            content_parts.append(md_t)
        except Exception as e:  # pragma: no cover
            logger.warning("Tavily basic error: %s", e)

        # 3Ô∏è‚É£ TAVILY ADVANCED (fallback)
        if (
            self.cfg.escalate_to_advanced
            and len(urls) < self.cfg.min_docs
        ):
            try:
                t_adv = self.tavily_advanced.invoke({"query": query})
                urls_a, md_a = self._process_tavily_results(
                    t_adv, self.cfg.max_content_length
                )
                urls.extend([u for u in urls_a if u not in urls])
                content_parts.append(md_a)
            except Exception as e:  # pragma: no cover
                logger.warning("Tavily advanced error: %s", e)

        if not urls:
            content = "üîç Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan ƒë·∫øn truy v·∫•n n√†y."
        else:
            content = "\n".join(filter(None, content_parts))

        logger.info("üîç Ho√†n t·∫•t t√¨m ki·∫øm ‚Äì %d ngu·ªìn", len(urls))

        # Cache
        self.cache.set(query, content, urls)
        return content, urls

    # ---------------------------------------------------------------------

    def search_with_sources(self, query: str) -> Tuple[str, List[str]]:
        """N·∫øu enable_llm ‚Üí t√≥m t·∫Øt; ng∆∞·ª£c l·∫°i tr·∫£ raw_content."""
        raw_content, urls = self.search_raw_results(query)
        if not self.llm or raw_content.startswith("üîç"):
            return raw_content, urls

        prompt = f"""B·∫°n l√† tr·ª£ l√Ω t√≥m t·∫Øt. H√£y tr·∫£ l·ªùi (TV) g·ªìm:
1. **T√≥m t·∫Øt** th√¥ng tin ch√≠nh.
2. **Ngu·ªìn**: li·ªát k√™ URL m·ªói d√≤ng.

---
{raw_content}
---"""
        try:
            summary = self.llm.invoke(prompt).content.strip()
            return summary, urls
        except Exception as e:  # pragma: no cover
            logger.warning("LLM error, tr·∫£ raw: %s", e)
            return raw_content, urls

    # Debug helpers
    def get_cache_stats(self) -> Dict[str, Any]:
        return {
            "cache_size": len(self.cache.store),
            "rate_limit_calls": len(self.rate_limiter.calls),
            "max_calls_per_minute": self.rate_limiter.max_calls,
        }

    def clear_cache(self):
        self.cache.store.clear()
        logger.info("üóëÔ∏è Cache cleared")


# ================== SINGLETON & LEGACY WRAPPERS ============================

_search_instance: Optional[OptimizedGoogleSearch] = None


def get_search_instance(config: SearchConfig = SearchConfig()) -> OptimizedGoogleSearch:
    """Tr·∫£ v·ªÅ singleton (d√πng chung trong c·∫£ app)."""
    global _search_instance
    if _search_instance is None:
        _search_instance = OptimizedGoogleSearch(config)
    return _search_instance


# --- Legacy aliases (gi·ªØ nguy√™n ch·ªØ k√Ω) -----------------------------------

def run_query_with_sources(query: str) -> Tuple[str, List[str]]:
    logger.warning("‚ö†Ô∏è Legacy function ‚Äì¬†n√™n migrate sang get_search_instance().search_with_sources")
    return get_search_instance().search_with_sources(query)


def get_raw_search_results(query: str) -> Tuple[str, List[str]]:
    return get_search_instance().search_raw_results(query)


def tavily_with_sources(query: str) -> Tuple[List[str], str]:
    logger.warning("‚ö†Ô∏è Legacy function ‚Äì¬†n√™n migrate sang search_with_sources")
    answer, urls = get_search_instance().search_with_sources(query)
    return urls, answer


# ============================== SELF‚ÄëTEST ==================================
if __name__ == "__main__":
    os.environ.setdefault("TAVILY_API_KEY", "DUMMY")
    os.environ.setdefault("SERPER_API_KEY", "DUMMY")
    cfg = SearchConfig(enable_llm=False, google_top_k=3)
    search = OptimizedGoogleSearch(cfg)
    for q in [
        "Nh·ªØng h·ªá qu·∫£n tr·ªã c∆° s·ªü d·ªØ li·ªáu n√†o ƒëang d·∫´n ƒë·∫ßu v·ªÅ hi·ªáu su·∫•t v√† t√≠nh nƒÉng trong nƒÉm 2025?",
        "ACID vs BASE database",
    ]:
        print("‚Äî" * 80)
        print("üîç", q)
        content, urls = search.search_raw_results(q)
        print(content[:800] + ("‚Ä¶" if len(content) > 800 else ""))
        print("\nNgu·ªìn:")
        for i, u in enumerate(urls, 1):
            print(f"{i}. {u}")
