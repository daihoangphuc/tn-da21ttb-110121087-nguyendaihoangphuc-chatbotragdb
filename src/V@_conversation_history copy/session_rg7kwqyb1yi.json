{
  "session_id": "session_rg7kwqyb1yi",
  "last_updated": "2025-05-09T22:01:59.265088",
  "messages": [
    {
      "role": "user",
      "content": "read data là gì trong hadoop?"
    },
    {
      "role": "assistant",
      "content": "## Đọc dữ liệu (Read Data) trong Hadoop\n\n**Đọc dữ liệu trong Hadoop là quá trình truy xuất và sử dụng dữ liệu được lưu trữ trong hệ thống tệp phân tán Hadoop (HDFS) để phục vụ cho các mục đích phân tích, xử lý và tính toán.**\n\n### Quá trình đọc dữ liệu từ HDFS\n\nQuá trình đọc dữ liệu từ HDFS bao gồm các bước sau:\n\n1.  **Client gửi yêu cầu mở file:** Client gọi phương thức `open()` ở `FileSystemObject` để mở file. **[Nguồn: src/data\\Li_thuyet_Hadoop.pdf, Trang 9, Phần list]**\n2.  **Truy vấn NameNode để lấy vị trí block:** `DistributedFileSystem` gọi `NameNode` để lấy vị trí của các blocks của file. `NameNode` trả về địa chỉ của các `DataNode` chứa bản sao của block đó. **[Nguồn: src/data\\Li_thuyet_Hadoop.pdf, Trang 9, Phần list]**\n3.  **Trả về FSDataInputStream:** Sau khi nhận được địa chỉ của các `DataNode`, một đối tượng `FSDataInputStream` được trả về cho client. `FSDataInputStream` chứa `DFSInputStream`. `DFSInputStream` quản lý I/O của `DataNode` và `NameNode`. **[Nguồn: src/data\\Li_thuyet_Hadoop.pdf, Trang 10, Phần list]**\n4.  **Đọc dữ liệu từ DataNode:** Client gọi phương thức `read()` ở `FSDataInputStream`, `DFSInputStream` kết nối với `DataNode` gần nhất để đọc block đầu tiên của file. Phương thức `read()` được lặp đi lặp lại nhiều lần cho đến cuối block. **[Nguồn: src/data\\Li_thuyet_Hadoop.pdf, Trang 10, Phần list]**\n5.  **Xử lý lỗi và chuyển sang DataNode khác:** Sau khi đọc xong, `DFSInputStream` ngắt kết nối và xác định `DataNode` cho block tiếp theo. Khi `DFSInputStream` đọc file, nếu có lỗi xảy ra nó sẽ chuyển sang `DataNode` khác gần nhất có chứa block đó. **[Nguồn: src/data\\Li_thuyet_Hadoop.pdf, Trang 10, Phần list]**\n6.  **Đóng kết nối:** Khi client đọc xong file, gọi `close()`. **[Nguồn: src/data\\Li_thuyet_Hadoop.pdf, Trang 10, Phần list]**\n\n### Các thành phần liên quan\n\n*   **HDFS (Hadoop Distributed File System):** Hệ thống lưu trữ phân tán của Hadoop, cung cấp khả năng lưu trữ và truy cập dữ liệu lớn. **[Nguồn: src/data\\Li_thuyet_Hadoop.pdf, Trang 3, Phần list]**\n*   **NameNode:** Quản lý không gian tên và metadata của hệ thống file. **[Nguồn: src/data\\Li_thuyet_Hadoop.pdf, Trang 3, Phần list]**\n*   **DataNode:** Lưu trữ dữ liệu thực tế và thực hiện các thao tác đọc/ghi dữ liệu. **[Nguồn: src/data\\Li_thuyet_Hadoop.pdf, Trang 3, Phần list]**\n*   **FSDataInputStream:** Cung cấp giao diện để đọc dữ liệu từ HDFS. **[Nguồn: src/data\\Li_thuyet_Hadoop.pdf, Trang 9, Phần list]**\n*   **DFSInputStream:** Quản lý I/O của DataNode và NameNode trong quá trình đọc dữ liệu. **[Nguồn: src/data\\Li_thuyet_Hadoop.pdf, Trang 9, Phần list]**\n\n### Ví dụ\n\nTrong thực tế, người dùng thường không làm việc trực tiếp với HDFS bằng các câu lệnh mà thường đọc dữ liệu thông qua Spark, ví dụ:\n\n```text\ndataFrame.write.save(\"<path_on_hadoop>\")\n```text\n\n**[Nguồn: src/data\\Li_thuyet_Hadoop.pdf, Trang 9, Phần list]**\n\n"
    },
    {
      "role": "user",
      "content": "Write data trong haddoop?"
    },
    {
      "role": "assistant",
      "content": "## Write data trong Hadoop\n\nQuá trình ghi dữ liệu (Write data) trong Hadoop bao gồm các bước sau:\n\n### Các bước ghi dữ liệu vào HDFS\n\n1.  **Client gửi yêu cầu tạo file:** Client gửi yêu cầu tạo file ở DistributedFileSystem APIs. **[Nguồn: src/data\\Li_thuyet_Hadoop.pdf, Trang 8, Phần list]**\n2.  **NameNode kiểm tra:** `DistributedFileSystem` yêu cầu tạo file ở `NameNode`. `NameNode` kiểm tra quyền của client và kiểm tra file mới có tồn tại hay không. **[Nguồn: src/data\\Li_thuyet_Hadoop.pdf, Trang 9, Phần list]**\n3.  **Trả về FSDataOutputStream:** `DistributedFileSystem` trả về `FSDataOutPutStream` cho client để ghi dữ liệu. `FSDataOutputStream` chứa `DFSOutputStream`, nó dùng để xử lý tương tác với `NameNode` và `DataNode`. **[Nguồn: src/data\\Li_thuyet_Hadoop.pdf, Trang 9, Phần list]**\n4.  **Ghi dữ liệu và phân chia packet:** Khi client ghi dữ liệu, `DFSOutputStream` chia dữ liệu thành các packet và đẩy nó vào hàng đợi `DataQueue`. `DataStreamer` sẽ nói với `NameNode` để phân bổ các block vào các datanode để lưu trữ các bản sao. **[Nguồn: src/data\\Li_thuyet_Hadoop.pdf, Trang 9, Phần list]**\n5.  **Tạo pipeline DataNode:** Các `DataNode` tạo thành pipeline, số datanode bằng số bản sao của file. `DataStream` gửi packet tới `DataNode` đầu tiên trong pipeline, datanode này sẽ chuyển tiếp packet lần lượt tới các `Datanode` trong pipeline. **[Nguồn: src/data\\Li_thuyet_Hadoop.pdf, Trang 9, Phần list]**\n6.  **Ack Queue:** `DFSOutputStream` có `Ack Queue` để duy trì các packet chưa được xác nhận bởi các `DataNode`. Packet ra khỏi ackqueue khi nhận được xác nhận từ tất cả các `DataNode`. **[Nguồn: src/data\\Li_thuyet_Hadoop.pdf, Trang 9, Phần list]**\n7.  **Kết thúc ghi dữ liệu:** Client gọi `close()` để kết thúc ghi dữ liệu, các packet còn lại được đẩy vào pipeline. **[Nguồn: src/data\\Li_thuyet_Hadoop.pdf, Trang 9, Phần list]**\n8.  **Thông báo hoàn thành:** Sau khi toàn bộ các packet được ghi vào các `DataNode`, thông báo hoàn thành ghi file. **[Nguồn: src/data\\Li_thuyet_Hadoop.pdf, Trang 9, Phần list]**\n\n### Các thành phần liên quan\n\n*   **HDFS (Hadoop Distributed File System):** Hệ thống lưu trữ phân tán của Hadoop. **[Nguồn: src/data\\Li_thuyet_Hadoop.pdf, Trang 3, Phần list]**\n*   **NameNode:** Quản lý không gian tên và metadata của hệ thống file. **[Nguồn: src/data\\Li_thuyet_Hadoop.pdf, Trang 3, Phần list]**\n*   **DataNode:** Lưu trữ dữ liệu thực tế. **[Nguồn: src/data\\Li_thuyet_Hadoop.pdf, Trang 3, Phần list]**\n*   **FSDataOutputStream:** Cung cấp giao diện để ghi dữ liệu vào HDFS. **[Nguồn: src/data\\Li_thuyet_Hadoop.pdf, Trang 9, Phần list]**\n*   **DFSOutputStream:** Xử lý tương tác với NameNode và DataNode trong quá trình ghi dữ liệu. **[Nguồn: src/data\\Li_thuyet_Hadoop.pdf, Trang 9, Phần list]**\n\n### Ví dụ\n\nVí dụ một trong những lệnh thao tác ghi trên hdfs:\n\n```text\nhdfs dfs -put <path_on_your_computer> <path_on_hadoop>\n```\n\n**[Nguồn: src/data\\Li_thuyet_Hadoop.pdf, Trang 9, Phần list]**\n\nTrong quá trình thực tế hầu như sẽ không làm việc trực tiếp với hệ thống file system của hadoop(HDFS) bằng câu lệnh, mà ta thường đọc, ghi qua spark, ví dụ:\n\n```text\ndataFrame.write.save(\"<path_on_hadoop>\")\n```\n\n**[Nguồn: src/data\\Li_thuyet_Hadoop.pdf, Trang 9, Phần list]**\n"
    }
  ]
}