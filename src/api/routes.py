from fastapi import (
    APIRouter,
    HTTPException,
    BackgroundTasks,
    UploadFile,
    File,
    Form,
    Path,
    Query,
)
from typing import List, Optional, Dict, Any
import os
import uuid
import shutil
import tempfile
import time
from datetime import datetime
from pydantic import BaseModel
from tqdm import tqdm

from src.app import RAGPipeline
from src.loaders import DocumentLoader


router = APIRouter()

# Kh·ªüi t·∫°o RAG Pipeline
pipeline = RAGPipeline()

# Th∆∞ m·ª•c l∆∞u d·ªØ li·ªáu c·ªë ƒë·ªãnh
DATA_DIR = "D:\\DATN\\V2\\src\\data"

# Tr·∫°ng th√°i indexing ƒë·ªÉ theo d√µi
indexing_tasks = {}


class QueryRequest(BaseModel):
    query: str


class SourceInfo(BaseModel):
    index: int
    source: str
    source_path: str
    file_type: str
    chunk_length: int
    chunk_word_count: int
    start_index: int
    chunk_count: int
    has_list_content: bool
    content_preview: str
    page_number: Optional[int] = None
    image_paths: Optional[List[str]] = []
    pdf_element_type: Optional[str] = None


class QueryResponse(BaseModel):
    response: str
    sources: List[SourceInfo]
    prompt: str
    model: str
    query: str
    temperature: float
    total_sources: int
    retrieval_time_ms: int
    llm_time_ms: int
    total_tokens: int
    related_images: Optional[List[str]] = []


class IndexResponse(BaseModel):
    task_id: str
    message: str


class DeleteFileResponse(BaseModel):
    deleted_file: str
    deleted_embeddings: int
    success: bool
    message: str


class TaskStatus(BaseModel):
    task_id: str
    status: str
    message: Optional[str] = None


class ProgressDetail(BaseModel):
    step: str
    step_name: str
    completed_steps: int
    total_steps: int
    progress_percent: float
    start_time: float
    current_time: float
    elapsed_time: float
    estimated_time_remaining: Optional[float] = None


class TaskProgress(BaseModel):
    task_id: str
    status: str
    message: str
    progress: Optional[ProgressDetail] = None


class FileInfo(BaseModel):
    name: str
    path: str
    size: int
    last_modified: float


@router.get("/")
async def root():
    return {"message": "API h·ªá th·ªëng RAG ƒëang ho·∫°t ƒë·ªông"}


@router.post("/query", response_model=QueryResponse, status_code=200)
async def query(request: QueryRequest):
    """API endpoint cho vi·ªác truy v·∫•n"""
    try:
        if not request.query or request.query.strip() == "":
            raise HTTPException(status_code=400, detail="C√¢u truy v·∫•n kh√¥ng ƒë∆∞·ª£c tr·ªëng")

        # G·ªçi pipeline.query ƒë·ªÉ l·∫•y k·∫øt qu·∫£ chi ti·∫øt
        result = pipeline.query(request.query)

        # Tr√≠ch xu·∫•t ƒë∆∞·ªùng d·∫´n ·∫£nh t·ª´ sources
        related_images = []
        for source in result.get("sources", []):
            if "image_paths" in source and source["image_paths"]:
                for img_path in source["image_paths"]:
                    if img_path not in related_images:
                        related_images.append(img_path)
            # Ki·ªÉm tra c·∫£ image_path (ƒë∆°n l·∫ª)
            if "image_path" in source and source["image_path"]:
                if source["image_path"] not in related_images:
                    related_images.append(source["image_path"])

        # Chuy·ªÉn ƒë·ªïi ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi th√†nh URL API
        image_urls = [
            f"/images/{os.path.basename(os.path.dirname(img_path))}/{os.path.basename(img_path)}"
            for img_path in related_images
        ]

        # Chu·∫©n b·ªã response v·ªõi ƒë·∫ßy ƒë·ªß th√¥ng tin
        response = QueryResponse(
            response=result.get("text", ""),
            sources=result.get("sources", []),
            prompt=result.get("prompt", ""),
            model=result.get("model", ""),
            query=result.get("query", request.query),
            temperature=result.get("temperature", 0.0),
            total_sources=result.get("total_sources", 0),
            retrieval_time_ms=round(result.get("retrieval_time", 0) * 1000),
            llm_time_ms=round(result.get("llm_time", 0) * 1000),
            total_tokens=result.get("total_tokens", 0),
            related_images=image_urls,
        )

        return response
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói khi x·ª≠ l√Ω truy v·∫•n: {str(e)}")


@router.post("/upload", response_model=IndexResponse)
async def upload_and_index(
    background_tasks: BackgroundTasks, files: List[UploadFile] = File(...)
):
    """API endpoint ƒë·ªÉ upload t√†i li·ªáu v√† index"""
    task_id = str(uuid.uuid4())

    # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i
    os.makedirs(DATA_DIR, exist_ok=True)

    # T·∫°o th∆∞ m·ª•c con theo th·ªùi gian ƒë·ªÉ tr√°nh tr√πng l·∫∑p
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    upload_dir = os.path.join(DATA_DIR, f"upload_{timestamp}")
    os.makedirs(upload_dir, exist_ok=True)

    try:
        # L∆∞u c√°c file ƒë∆∞·ª£c upload v√†o th∆∞ m·ª•c d·ªØ li·ªáu
        file_paths = []
        for file in files:
            file_path = os.path.join(upload_dir, file.filename)
            with open(file_path, "wb") as buffer:
                shutil.copyfileobj(file.file, buffer)
            file_paths.append(file_path)

        # Kh·ªüi t·∫°o tr·∫°ng th√°i task v·ªõi progress chi ti·∫øt
        current_time = time.time()
        indexing_tasks[task_id] = {
            "status": "running",
            "message": "ƒêang l∆∞u file v√† chu·∫©n b·ªã x·ª≠ l√Ω...",
            "files": file_paths,
            "directory": upload_dir,
            "progress": {
                "step": "init",
                "step_name": "Kh·ªüi t·∫°o",
                "completed_steps": 0,
                "total_steps": 4,
                "progress_percent": 0.0,
                "start_time": current_time,
                "current_time": current_time,
                "elapsed_time": 0.0,
                "estimated_time_remaining": None,
            },
        }

        # Th·ª±c hi·ªán indexing trong background v·ªõi stream progress
        background_tasks.add_task(
            _process_indexing_with_progress, task_id, upload_dir, False
        )

        return {
            "task_id": task_id,
            "message": f"ƒê√£ upload v√† b·∫Øt ƒë·∫ßu qu√° tr√¨nh indexing {len(files)} file",
        }
    except Exception as e:
        # X√≥a th∆∞ m·ª•c n·∫øu c√≥ l·ªói
        shutil.rmtree(upload_dir, ignore_errors=True)
        raise HTTPException(
            status_code=500, detail=f"L·ªói khi x·ª≠ l√Ω upload v√† indexing: {str(e)}"
        )


@router.post("/index/files", response_model=IndexResponse)
async def index_files(
    background_tasks: BackgroundTasks, files: List[UploadFile] = File(...)
):
    """API endpoint ƒë·ªÉ index d·ªØ li·ªáu t·ª´ c√°c file ƒë∆∞·ª£c upload"""
    task_id = str(uuid.uuid4())
    temp_dir = tempfile.mkdtemp()

    try:
        # L∆∞u c√°c file ƒë∆∞·ª£c upload v√†o th∆∞ m·ª•c t·∫°m
        for file in files:
            file_path = os.path.join(temp_dir, file.filename)
            with open(file_path, "wb") as buffer:
                shutil.copyfileobj(file.file, buffer)

        # Kh·ªüi t·∫°o tr·∫°ng th√°i task v·ªõi progress chi ti·∫øt
        current_time = time.time()
        indexing_tasks[task_id] = {
            "status": "running",
            "message": "ƒêang x·ª≠ l√Ω...",
            "progress": {
                "step": "init",
                "step_name": "Kh·ªüi t·∫°o",
                "completed_steps": 0,
                "total_steps": 4,
                "progress_percent": 0.0,
                "start_time": current_time,
                "current_time": current_time,
                "elapsed_time": 0.0,
                "estimated_time_remaining": None,
            },
        }

        # Th·ª±c hi·ªán indexing trong background
        background_tasks.add_task(
            _process_indexing_with_progress, task_id, temp_dir, True
        )

        return {
            "task_id": task_id,
            "message": f"ƒê√£ b·∫Øt ƒë·∫ßu qu√° tr√¨nh indexing {len(files)} file",
        }
    except Exception as e:
        # X√≥a th∆∞ m·ª•c t·∫°m n·∫øu c√≥ l·ªói
        shutil.rmtree(temp_dir, ignore_errors=True)
        raise HTTPException(status_code=500, detail=f"L·ªói khi x·ª≠ l√Ω indexing: {str(e)}")


@router.post("/index/path", response_model=IndexResponse)
async def index_directory(
    background_tasks: BackgroundTasks, directory: str = Form(...)
):
    """API endpoint ƒë·ªÉ index d·ªØ li·ªáu t·ª´ m·ªôt th∆∞ m·ª•c"""
    task_id = str(uuid.uuid4())

    try:
        # Ki·ªÉm tra th∆∞ m·ª•c c√≥ t·ªìn t·∫°i kh√¥ng
        if not os.path.exists(directory) or not os.path.isdir(directory):
            raise HTTPException(
                status_code=400, detail=f"Th∆∞ m·ª•c kh√¥ng t·ªìn t·∫°i: {directory}"
            )

        # Kh·ªüi t·∫°o tr·∫°ng th√°i task v·ªõi progress chi ti·∫øt
        current_time = time.time()
        indexing_tasks[task_id] = {
            "status": "running",
            "message": "ƒêang x·ª≠ l√Ω...",
            "progress": {
                "step": "init",
                "step_name": "Kh·ªüi t·∫°o",
                "completed_steps": 0,
                "total_steps": 4,
                "progress_percent": 0.0,
                "start_time": current_time,
                "current_time": current_time,
                "elapsed_time": 0.0,
                "estimated_time_remaining": None,
            },
        }

        # Th·ª±c hi·ªán indexing trong background
        background_tasks.add_task(
            _process_indexing_with_progress, task_id, directory, False
        )

        return {
            "task_id": task_id,
            "message": f"ƒê√£ b·∫Øt ƒë·∫ßu qu√° tr√¨nh indexing t·ª´ th∆∞ m·ª•c {directory}",
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói khi x·ª≠ l√Ω indexing: {str(e)}")


@router.get("/index/status/{task_id}", response_model=TaskStatus)
async def get_index_status(task_id: str):
    """L·∫•y tr·∫°ng th√°i c·ªßa task indexing"""
    if task_id not in indexing_tasks:
        raise HTTPException(status_code=404, detail="Task kh√¥ng t·ªìn t·∫°i")

    return {
        "task_id": task_id,
        "status": indexing_tasks[task_id]["status"],
        "message": indexing_tasks[task_id]["message"],
    }


@router.get("/index/progress/{task_id}", response_model=TaskProgress)
async def get_index_progress(task_id: str):
    """L·∫•y th√¥ng tin chi ti·∫øt v·ªÅ ti·∫øn tr√¨nh x·ª≠ l√Ω"""
    if task_id not in indexing_tasks:
        raise HTTPException(status_code=404, detail="Task kh√¥ng t·ªìn t·∫°i")

    task = indexing_tasks[task_id]

    # C·∫≠p nh·∫≠t th·ªùi gian hi·ªán t·∫°i v√† t√≠nh to√°n th·ªùi gian ƒë√£ tr√¥i qua
    if "progress" in task:
        current_time = time.time()
        task["progress"]["current_time"] = current_time
        task["progress"]["elapsed_time"] = current_time - task["progress"]["start_time"]

    return {
        "task_id": task_id,
        "status": task["status"],
        "message": task["message"],
        "progress": task.get("progress"),
    }


@router.delete("/index")
async def delete_index():
    """API endpoint ƒë·ªÉ x√≥a index"""
    try:
        pipeline.delete_index()
        return {"message": "ƒê√£ x√≥a index th√†nh c√¥ng"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói khi x√≥a index: {str(e)}")


@router.get("/uploads")
async def list_uploads():
    """API endpoint ƒë·ªÉ li·ªát k√™ c√°c th∆∞ m·ª•c upload"""
    try:
        if not os.path.exists(DATA_DIR):
            return {"uploads": []}

        # L·∫•y danh s√°ch th∆∞ m·ª•c upload
        uploads = []
        for item in os.listdir(DATA_DIR):
            item_path = os.path.join(DATA_DIR, item)
            if os.path.isdir(item_path) and item.startswith("upload_"):
                # L·∫•y th√¥ng tin th∆∞ m·ª•c
                files = [
                    f
                    for f in os.listdir(item_path)
                    if os.path.isfile(os.path.join(item_path, f))
                ]
                uploads.append(
                    {
                        "name": item,
                        "path": item_path,
                        "files": files,
                        "file_count": len(files),
                    }
                )

        return {"uploads": uploads}
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"L·ªói khi li·ªát k√™ uploads: {str(e)}"
        )


@router.get("/files", response_model=List[FileInfo])
async def list_files():
    """API endpoint ƒë·ªÉ li·ªát k√™ c√°c file trong t·∫•t c·∫£ th∆∞ m·ª•c upload"""
    try:
        if not os.path.exists(DATA_DIR):
            return []

        # Danh s√°ch file
        files_info = []

        # Duy·ªát qua c√°c th∆∞ m·ª•c upload
        for folder in os.listdir(DATA_DIR):
            folder_path = os.path.join(DATA_DIR, folder)

            if os.path.isdir(folder_path) and folder.startswith("upload_"):
                # Duy·ªát qua c√°c file trong th∆∞ m·ª•c
                for file_name in os.listdir(folder_path):
                    file_path = os.path.join(folder_path, file_name)

                    if os.path.isfile(file_path):
                        # L·∫•y th√¥ng tin file
                        file_stat = os.stat(file_path)

                        files_info.append(
                            FileInfo(
                                name=file_name,
                                path=file_path,
                                size=file_stat.st_size,
                                last_modified=file_stat.st_mtime,
                            )
                        )

        # S·∫Øp x·∫øp theo th·ªùi gian s·ª≠a ƒë·ªïi m·ªõi nh·∫•t
        files_info.sort(key=lambda x: x.last_modified, reverse=True)

        return files_info
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói khi li·ªát k√™ files: {str(e)}")


@router.delete("/files/{file_name:path}", response_model=DeleteFileResponse)
async def delete_file(
    file_name: str = Path(
        ..., description="T√™n file c·∫ßn x√≥a (c√≥ ph√¢n bi·ªát ch·ªØ hoa/th∆∞·ªùng)"
    ),
    upload_dir: Optional[str] = Query(
        None, description="Th∆∞ m·ª•c upload c·ª• th·ªÉ (n·∫øu c√≥)"
    ),
):
    """API endpoint ƒë·ªÉ x√≥a file v√† embedding t∆∞∆°ng ·ª©ng"""
    try:
        # T√¨m file trong th∆∞ m·ª•c d·ªØ li·ªáu
        file_path = None

        if upload_dir:
            # N·∫øu ch·ªâ ƒë·ªãnh th∆∞ m·ª•c upload c·ª• th·ªÉ
            dir_path = os.path.join(DATA_DIR, upload_dir)
            if os.path.exists(dir_path) and os.path.isdir(dir_path):
                temp_path = os.path.join(dir_path, file_name)
                if os.path.exists(temp_path) and os.path.isfile(temp_path):
                    file_path = temp_path
        else:
            # Duy·ªát qua t·∫•t c·∫£ th∆∞ m·ª•c upload
            for folder in os.listdir(DATA_DIR):
                folder_path = os.path.join(DATA_DIR, folder)

                if os.path.isdir(folder_path) and folder.startswith("upload_"):
                    temp_path = os.path.join(folder_path, file_name)
                    if os.path.exists(temp_path) and os.path.isfile(temp_path):
                        file_path = temp_path
                        break

        if not file_path:
            raise HTTPException(
                status_code=404, detail=f"Kh√¥ng t√¨m th·∫•y file: {file_name}"
            )

        # X√≥a c√°c embedding li√™n quan trong vectorstore
        deleted_count = pipeline.delete_file(file_path)

        # X√≥a file th·ª±c t·∫ø
        os.remove(file_path)

        return {
            "deleted_file": file_path,
            "deleted_embeddings": deleted_count,
            "success": True,
            "message": f"ƒê√£ x√≥a file: {file_name} v√† {deleted_count} embedding li√™n quan",
        }

    except FileNotFoundError:
        raise HTTPException(status_code=404, detail=f"Kh√¥ng t√¨m th·∫•y file: {file_name}")
    except PermissionError:
        raise HTTPException(
            status_code=403, detail=f"Kh√¥ng c√≥ quy·ªÅn x√≥a file: {file_name}"
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói khi x√≥a file: {str(e)}")


@router.post("/reindex", response_model=IndexResponse)
async def reindex_all_documents(background_tasks: BackgroundTasks):
    """API endpoint ƒë·ªÉ t√°i ch·ªâ m·ª•c to√†n b·ªô t√†i li·ªáu hi·ªán c√≥"""
    task_id = str(uuid.uuid4())

    try:
        # Ki·ªÉm tra c√≥ d·ªØ li·ªáu kh√¥ng
        if not os.path.exists(DATA_DIR):
            raise HTTPException(
                status_code=400, detail="Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c d·ªØ li·ªáu"
            )

        # T√¨m t·∫•t c·∫£ th∆∞ m·ª•c upload
        upload_dirs = []
        for item in os.listdir(DATA_DIR):
            item_path = os.path.join(DATA_DIR, item)
            if os.path.isdir(item_path) and item.startswith("upload_"):
                upload_dirs.append(item_path)

        if not upload_dirs:
            raise HTTPException(
                status_code=400, detail="Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c upload n√†o"
            )

        # Kh·ªüi t·∫°o tr·∫°ng th√°i task v·ªõi progress chi ti·∫øt
        current_time = time.time()
        indexing_tasks[task_id] = {
            "status": "running",
            "message": "ƒêang chu·∫©n b·ªã x√≥a index hi·ªán t·∫°i v√† t√°i ch·ªâ m·ª•c...",
            "progress": {
                "step": "init",
                "step_name": "Kh·ªüi t·∫°o",
                "completed_steps": 0,
                "total_steps": 4,
                "progress_percent": 0.0,
                "start_time": current_time,
                "current_time": current_time,
                "elapsed_time": 0.0,
                "estimated_time_remaining": None,
            },
        }

        # Th·ª±c hi·ªán x√≥a index hi·ªán t·∫°i v√† t√°i ch·ªâ m·ª•c trong background
        async def _reindex_all():
            try:
                # 1. X√≥a index hi·ªán t·∫°i
                _update_progress(
                    task_id,
                    "delete_index",
                    "X√≥a index hi·ªán t·∫°i",
                    1,
                    "ƒêang x√≥a index hi·ªán t·∫°i...",
                    0.1,
                )
                pipeline.delete_index()

                # 2. L·∫∑p qua t·ª´ng th∆∞ m·ª•c upload ƒë·ªÉ index l·∫°i
                for i, upload_dir in enumerate(upload_dirs):
                    progress = 0.1 + 0.8 * (i / len(upload_dirs))
                    _update_progress(
                        task_id,
                        "indexing",
                        f"Index th∆∞ m·ª•c {i+1}/{len(upload_dirs)}",
                        2,
                        f"ƒêang index th∆∞ m·ª•c: {os.path.basename(upload_dir)}",
                        progress,
                    )
                    await _process_indexing_with_progress(
                        f"{task_id}_{i}", upload_dir, False
                    )

                # 3. Ho√†n th√†nh
                _update_progress(
                    task_id,
                    "completed",
                    "Ho√†n th√†nh",
                    4,
                    "ƒê√£ t√°i ch·ªâ m·ª•c t·∫•t c·∫£ t√†i li·ªáu",
                    1.0,
                )
                indexing_tasks[task_id]["status"] = "completed"
                indexing_tasks[task_id][
                    "message"
                ] = "ƒê√£ t√°i ch·ªâ m·ª•c th√†nh c√¥ng t·∫•t c·∫£ t√†i li·ªáu"

            except Exception as e:
                error_message = f"L·ªói khi t√°i ch·ªâ m·ª•c: {str(e)}"
                print(f"‚ùå {error_message}")
                indexing_tasks[task_id]["status"] = "failed"
                indexing_tasks[task_id]["message"] = error_message

        # Th·ª±c hi·ªán t√°i ch·ªâ m·ª•c trong background
        background_tasks.add_task(_reindex_all)

        return {
            "task_id": task_id,
            "message": f"ƒê√£ b·∫Øt ƒë·∫ßu t√°i ch·ªâ m·ª•c t·∫•t c·∫£ t√†i li·ªáu t·ª´ {len(upload_dirs)} th∆∞ m·ª•c upload",
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói khi t√°i ch·ªâ m·ª•c: {str(e)}")


async def _process_indexing_with_progress(
    task_id: str, data_dir: str, is_temp_dir: bool = True
):
    """H√†m x·ª≠ l√Ω indexing trong background v·ªõi c·∫≠p nh·∫≠t ti·∫øn tr√¨nh chi ti·∫øt"""
    try:
        # ƒê·∫£m b·∫£o task ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o
        if task_id not in indexing_tasks:
            indexing_tasks[task_id] = {
                "status": "running",
                "message": "ƒêang x·ª≠ l√Ω...",
                "progress": {
                    "step": "init",
                    "step_name": "Kh·ªüi t·∫°o",
                    "completed_steps": 0,
                    "total_steps": 4,
                    "progress_percent": 0.0,
                    "start_time": time.time(),
                    "current_time": time.time(),
                    "elapsed_time": 0.0,
                    "estimated_time_remaining": None,
                },
            }

        # In th√¥ng tin c·∫•u h√¨nh
        from src.config import (
            EMBEDDING_MODEL,
            DOCUMENT_LOADER_MAX_WORKERS,
            QDRANT_BATCH_SIZE,
            CLUSTERING_BATCH_SIZE,
        )

        print(f"üìÑ C·∫•u h√¨nh x·ª≠ l√Ω:")
        print(f"  - Model embedding: {EMBEDDING_MODEL}")
        print(f"  - Document loader workers: {DOCUMENT_LOADER_MAX_WORKERS}")
        print(f"  - Clustering batch size: {CLUSTERING_BATCH_SIZE}")
        print(f"  - Qdrant batch size: {QDRANT_BATCH_SIZE}")

        # T·∫°o thanh ti·∫øn tr√¨nh t·ªïng th·ªÉ
        overall_progress = tqdm(total=100, desc="T·ªïng qu√° tr√¨nh x·ª≠ l√Ω", unit="%")

        # B∆∞·ªõc 1: Load t√†i li·ªáu (25%)
        _update_progress(
            task_id, "loading", "Load t√†i li·ªáu", 0, "ƒêang load t√†i li·ªáu...", 0.1
        )
        overall_progress.update(5)

        documents = DocumentLoader.load_documents(data_dir)

        _update_progress(
            task_id,
            "loading",
            "Load t√†i li·ªáu",
            1,
            f"ƒê√£ load {len(documents)} t√†i li·ªáu",
            0.25,
        )
        overall_progress.update(20)

        # Ph√¢n lo·∫°i t√†i li·ªáu theo lo·∫°i
        from src.config import SQL_FILE_EXTENSIONS
        from src.utils import get_file_extension

        # Ph√¢n chia t√†i li·ªáu theo lo·∫°i
        sql_docs = []
        regular_docs = []

        for doc in documents:
            # Ph√°t hi·ªán t√†i li·ªáu SQL d·ª±a tr√™n ph·∫ßn m·ªü r·ªông file
            ext = get_file_extension(doc.metadata.get("source_path", ""))
            if ext in SQL_FILE_EXTENSIONS:
                sql_docs.append(doc)
            else:
                regular_docs.append(doc)

        print(
            f"‚ÑπÔ∏è T·ªïng s·ªë t√†i li·ªáu: {len(documents)} (SQL: {len(sql_docs)}, Th√¥ng th∆∞·ªùng: {len(regular_docs)})"
        )

        # B∆∞·ªõc 2: Chunking (25%)
        _update_progress(
            task_id,
            "chunking",
            "Chia nh·ªè t√†i li·ªáu",
            1,
            "ƒêang chia nh·ªè t√†i li·ªáu...",
            0.3,
        )
        overall_progress.update(5)

        # X·ª≠ l√Ω t√†i li·ªáu th√¥ng th∆∞·ªùng
        chunks = []
        if regular_docs:
            regular_chunks = pipeline.document_processor.chunk_documents(regular_docs)
            chunks.extend(regular_chunks)

        # X·ª≠ l√Ω t√†i li·ªáu SQL n·∫øu c√≥
        if sql_docs:
            sql_chunks = pipeline.sql_processor.process_sql_documents(sql_docs)
            chunks.extend(sql_chunks)

        _update_progress(
            task_id,
            "chunking",
            "Chia nh·ªè t√†i li·ªáu",
            2,
            f"ƒê√£ chia th√†nh {len(chunks)} chunks",
            0.5,
        )
        overall_progress.update(15)

        # B∆∞·ªõc 3: Clustering & merging (25%)
        _update_progress(
            task_id,
            "clustering",
            "Ph√¢n c·ª•m v√† g·ªôp chunks",
            2,
            "ƒêang ph√¢n c·ª•m v√† g·ªôp chunks...",
            0.6,
        )
        overall_progress.update(10)

        # Ch·ªâ √°p d·ª•ng clustering cho t√†i li·ªáu th√¥ng th∆∞·ªùng
        merged_docs = []
        if regular_docs:
            regular_chunks_to_merge = [
                chunk for chunk in chunks if "sql_type" not in chunk.metadata
            ]
            if regular_chunks_to_merge:
                merged_regular_docs = pipeline.document_processor.cluster_and_merge(
                    regular_chunks_to_merge
                )
                merged_docs.extend(merged_regular_docs)

        # Th√™m c√°c chunk SQL (kh√¥ng c·∫ßn clustering)
        sql_chunks_to_add = [chunk for chunk in chunks if "sql_type" in chunk.metadata]
        merged_docs.extend(sql_chunks_to_add)

        _update_progress(
            task_id,
            "clustering",
            "Ph√¢n c·ª•m v√† g·ªôp chunks",
            3,
            f"ƒê√£ g·ªôp th√†nh {len(merged_docs)} t√†i li·ªáu",
            0.75,
        )
        overall_progress.update(15)

        # B∆∞·ªõc 4: Upload v√†o vector database (25%)
        _update_progress(
            task_id,
            "vectorizing",
            "Upload v√†o vector database",
            3,
            "ƒêang upload v√†o vector database...",
            0.8,
        )
        overall_progress.update(15)

        pipeline.vector_store_manager.upload_documents(merged_docs)

        # Ho√†n th√†nh
        _update_progress(
            task_id, "completed", "Ho√†n th√†nh", 4, "ƒê√£ ho√†n th√†nh indexing", 1.0
        )

        # Kh·ªüi t·∫°o l·∫°i vectorstore ƒë·ªÉ √°p d·ª•ng thay ƒë·ªïi
        pipeline._reinitialize_vectorstore()

        # C·∫≠p nh·∫≠t ph·∫ßn c√≤n l·∫°i c·ªßa thanh ti·∫øn tr√¨nh (ƒë·∫£m b·∫£o kh√¥ng v∆∞·ª£t qu√° 100%)
        current_progress = overall_progress.n
        if current_progress < 100:
            overall_progress.update(100 - current_progress)

        overall_progress.close()

        # C·∫≠p nh·∫≠t tr·∫°ng th√°i task
        indexing_tasks[task_id]["status"] = "completed"
        indexing_tasks[task_id]["message"] = "ƒê√£ ho√†n th√†nh indexing"

        # X√≥a th∆∞ m·ª•c t·∫°m n·∫øu c·∫ßn
        if is_temp_dir:
            shutil.rmtree(data_dir, ignore_errors=True)

    except Exception as e:
        error_message = f"L·ªói khi indexing: {str(e)}"
        print(f"‚ùå {error_message}")
        if task_id in indexing_tasks:
            indexing_tasks[task_id]["status"] = "failed"
            indexing_tasks[task_id]["message"] = error_message

        # X√≥a th∆∞ m·ª•c t·∫°m n·∫øu c√≥ l·ªói v√† l√† th∆∞ m·ª•c t·∫°m
        if is_temp_dir:
            shutil.rmtree(data_dir, ignore_errors=True)


def _update_progress(
    task_id: str,
    step: str,
    step_name: str,
    completed_steps: int,
    message: str,
    progress_percent: float,
):
    """C·∫≠p nh·∫≠t th√¥ng tin ti·∫øn tr√¨nh"""
    if task_id not in indexing_tasks:
        return

    task = indexing_tasks[task_id]
    current_time = time.time()

    # N·∫øu progress ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o
    if "progress" not in task:
        task["progress"] = {
            "step": step,
            "step_name": step_name,
            "completed_steps": completed_steps,
            "total_steps": 4,
            "progress_percent": progress_percent,
            "start_time": current_time,
            "current_time": current_time,
            "elapsed_time": 0.0,
            "estimated_time_remaining": None,
        }
    else:
        # C·∫≠p nh·∫≠t th√¥ng tin ti·∫øn tr√¨nh
        elapsed_time = current_time - task["progress"]["start_time"]

        # ∆Ø·ªõc t√≠nh th·ªùi gian c√≤n l·∫°i (n·∫øu ti·∫øn ƒë·ªô > 0)
        estimated_remaining = None
        if progress_percent > 0:
            estimated_remaining = (elapsed_time / progress_percent) * (
                1 - progress_percent
            )

        task["progress"].update(
            {
                "step": step,
                "step_name": step_name,
                "completed_steps": completed_steps,
                "progress_percent": progress_percent,
                "current_time": current_time,
                "elapsed_time": elapsed_time,
                "estimated_time_remaining": estimated_remaining,
            }
        )

    # C·∫≠p nh·∫≠t th√¥ng ƒëi·ªáp
    task["message"] = message
