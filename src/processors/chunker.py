import numpy as np
from typing import List
from langchain.schema import Document
from tqdm import tqdm

# from langchain_experimental.text_splitter import SemanticChunker
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sklearn.cluster import AgglomerativeClustering

from src.config import (
    CHUNK_BUFFER_SIZE,
    CHUNK_BREAKPOINT_THRESHOLD_TYPE,
    CHUNK_BREAKPOINT_THRESHOLD_AMOUNT,
    CLUSTER_DISTANCE_THRESHOLD,
    CHUNK_SIZE,
    CHUNK_OVERLAP,
    CHUNK_SEPARATORS,
    MIN_CHUNK_SIZE,
    MIN_CHUNK_CHARACTERS,
)
from src.utils import measure_time, print_document_info


class DocumentProcessor:
    """Lá»›p xá»­ lÃ½ chunking vÃ  clustering tÃ i liá»‡u"""

    def __init__(self, embeddings):
        """Khá»Ÿi táº¡o vá»›i embedding model Ä‘Ã£ cho"""
        self.embeddings = embeddings
        # PhÆ°Æ¡ng phÃ¡p chunking cÅ© sá»­ dá»¥ng SemanticChunker
        # self.chunker = SemanticChunker(
        #     embeddings=self.embeddings,
        #     buffer_size=CHUNK_BUFFER_SIZE,
        #     breakpoint_threshold_type=CHUNK_BREAKPOINT_THRESHOLD_TYPE,
        #     breakpoint_threshold_amount=CHUNK_BREAKPOINT_THRESHOLD_AMOUNT,
        #     add_start_index=True,
        # )
        # PhÆ°Æ¡ng phÃ¡p chunking má»›i sá»­ dá»¥ng RecursiveCharacterTextSplitter
        self.chunker = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
            # separators cho cÃ¡c nguá»“n chuyÃªn vá» lÄ©nh vá»±c cÆ¡ sá»Ÿ dá»¯ liá»‡u
            separators=CHUNK_SEPARATORS,
            length_function=len,
        )

    @measure_time
    def chunk_documents(self, docs: List[Document]) -> List[Document]:
        """Chia tÃ i liá»‡u thÃ nh cÃ¡c chunk cÃ³ ngá»¯ cáº£nh gáº§n nhau"""
        print("â³ Äang chunk tÃ i liá»‡u...")

        # Kiá»ƒm tra sá»‘ lÆ°á»£ng tÃ i liá»‡u vÃ  Ä‘iá»u chá»‰nh chunking náº¿u cáº§n
        doc_count = len(docs)
        print(f"â„¹ï¸ Sá»‘ lÆ°á»£ng tÃ i liá»‡u cáº§n chia: {doc_count}")

        total_text_length = sum(len(doc.page_content) for doc in docs)
        avg_doc_length = total_text_length / doc_count if doc_count > 0 else 0
        print(
            f"â„¹ï¸ Tá»•ng Ä‘á»™ dÃ i ná»™i dung: {total_text_length} kÃ½ tá»± (trung bÃ¬nh: {avg_doc_length:.1f} kÃ½ tá»±/tÃ i liá»‡u)"
        )

        # Äiá»u chá»‰nh cáº¥u hÃ¬nh chunking náº¿u tÃ i liá»‡u quÃ¡ dÃ i
        chunk_size = CHUNK_SIZE
        chunk_overlap = CHUNK_OVERLAP

        if avg_doc_length > 10000:  # Äiá»u chá»‰nh cho tÃ i liá»‡u dÃ i
            chunk_size = min(2000, chunk_size * 1.5)
            chunk_overlap = min(300, chunk_overlap * 1.5)
            print(
                f"â„¹ï¸ Äiá»u chá»‰nh chunking cho tÃ i liá»‡u dÃ i: chunk_size={chunk_size}, chunk_overlap={chunk_overlap}"
            )
            # Cáº­p nháº­t láº¡i chunker
            self.chunker = RecursiveCharacterTextSplitter(
                chunk_size=int(chunk_size),
                chunk_overlap=int(chunk_overlap),
                separators=CHUNK_SEPARATORS,
            )

        # PhÆ°Æ¡ng phÃ¡p chunking má»›i
        chunks = []
        for doc in tqdm(docs, desc="Chunking documents", unit="doc"):
            # TÃ¡ch vÄƒn báº£n thÃ nh cÃ¡c chunk
            text_chunks = self.chunker.split_text(doc.page_content)

            # Lá»c cÃ¡c chunk quÃ¡ ngáº¯n
            filtered_chunks = []
            for chunk in text_chunks:
                # Lá»c dá»±a trÃªn sá»‘ tá»« vÃ  sá»‘ kÃ½ tá»±
                num_words = len(chunk.split())
                num_chars = len(chunk)

                if num_words >= MIN_CHUNK_SIZE and num_chars >= MIN_CHUNK_CHARACTERS:
                    filtered_chunks.append(chunk)

            # BÃ¡o cÃ¡o thá»‘ng kÃª chunking
            if len(text_chunks) > 0:
                filtered_rate = len(filtered_chunks) / len(text_chunks) * 100
                if filtered_rate < 100:
                    print(
                        f"  - ÄÃ£ lá»c chunks cho '{doc.metadata.get('source', 'unknown')}': {len(filtered_chunks)}/{len(text_chunks)} chunks giá»¯ láº¡i ({filtered_rate:.1f}%)"
                    )

            # Chuyá»ƒn Ä‘á»•i text chunks thÃ nh Document objects
            doc_chunks = [
                Document(
                    page_content=chunk,
                    metadata={
                        **doc.metadata,
                        "start_index": doc.page_content.find(chunk),
                        "chunk_length": len(chunk),
                        "chunk_word_count": len(chunk.split()),
                    },
                )
                for chunk in filtered_chunks
            ]
            chunks.extend(doc_chunks)

        # TÃ­nh kÃ­ch thÆ°á»›c trung bÃ¬nh cá»§a chunks
        avg_chunk_length = (
            sum(len(c.page_content) for c in chunks) / len(chunks) if chunks else 0
        )
        avg_chunk_words = (
            sum(len(c.page_content.split()) for c in chunks) / len(chunks)
            if chunks
            else 0
        )

        print(
            f"â„¹ï¸ Thá»‘ng kÃª chunks: {len(chunks)} chunks (trung bÃ¬nh {avg_chunk_length:.1f} kÃ½ tá»±, {avg_chunk_words:.1f} tá»«/chunk)"
        )
        print_document_info(chunks, "Káº¿t quáº£ chunking")
        return chunks

    @measure_time
    def cluster_and_merge(self, chunks: List[Document]) -> List[Document]:
        """NhÃ³m vÃ  gá»™p cÃ¡c chunk liÃªn quan láº¡i vá»›i nhau"""
        print("â³ Äang embed & cluster...")

        # Tá»‘i Æ°u kÃ­ch thÆ°á»›c danh sÃ¡ch chunks náº¿u quÃ¡ lá»›n
        if len(chunks) > 1000:
            print(
                f"âš ï¸ Sá»‘ lÆ°á»£ng chunks quÃ¡ lá»›n ({len(chunks)}), chá»‰ xá»­ lÃ½ 1000 chunks Ä‘áº§u tiÃªn"
            )
            chunks = chunks[:1000]

        # Láº¥y embedding vá»›i tqdm Ä‘á»ƒ hiá»ƒn thá»‹ tiáº¿n trÃ¬nh
        print("â³ Äang tÃ­nh toÃ¡n embeddings...")
        chunk_texts = [c.page_content for c in chunks]

        # Sá»­ dá»¥ng batching Ä‘á»ƒ tÃ­nh embeddings hiá»‡u quáº£ hÆ¡n - tÄƒng batch_size lÃªn 64
        batch_size = 64  # TÄƒng tá»« 32 lÃªn 64
        all_embeddings = []

        for i in tqdm(
            range(0, len(chunk_texts), batch_size),
            desc="Embedding chunks",
            unit="batch",
        ):
            batch = chunk_texts[i : i + batch_size]
            batch_embeddings = self.embeddings.embed_documents(batch)
            all_embeddings.extend(batch_embeddings)

        vecs = np.array(all_embeddings)

        # Clustering vá»›i Agglomerative Clustering - giá»›i háº¡n sá»‘ lÆ°á»£ng clusters
        print("â³ Äang thá»±c hiá»‡n clustering...")
        # Tá»‘i Æ°u clustering threshold náº¿u sá»‘ lÆ°á»£ng chunks lá»›n
        threshold = CLUSTER_DISTANCE_THRESHOLD
        if len(chunks) > 500:
            threshold = CLUSTER_DISTANCE_THRESHOLD * 1.2
            print(f"âš ï¸ Äiá»u chá»‰nh threshold lÃªn {threshold} do cÃ³ nhiá»u chunks")

        labels = AgglomerativeClustering(
            n_clusters=None, distance_threshold=threshold
        ).fit_predict(vecs)

        # NhÃ³m cÃ¡c chunk theo cluster
        clustered = {}
        for lbl, c in zip(labels, chunks):
            clustered.setdefault(lbl, []).append(c)

        # Kiá»ƒm tra vÃ  in thÃ´ng tin vá» clusters
        print(f"ğŸ‘‰ ÄÃ£ phÃ¢n thÃ nh {len(clustered)} clusters")

        # Lá»c bá» cÃ¡c clusters quÃ¡ nhá» (chá»‰ 1 chunk) náº¿u cÃ³ quÃ¡ nhiá»u clusters
        if len(clustered) > 200:
            original_count = len(clustered)
            clustered = {k: v for k, v in clustered.items() if len(v) > 1}
            print(
                f"ğŸ‘‰ ÄÃ£ lá»c cÃ¡c clusters nhá»: tá»« {original_count} xuá»‘ng {len(clustered)} clusters"
            )

        # Sáº¯p xáº¿p vÃ  gá»™p cÃ¡c chunk trong cÃ¹ng má»™t cluster
        merged_docs = []
        for group_id in tqdm(clustered.keys(), desc="Merging clusters", unit="cluster"):
            group = clustered[group_id]
            group.sort(key=lambda d: d.metadata.get("start_index", 0))
            merged_content = "\n".join(d.page_content for d in group)

            # Láº¥y metadata tá»« pháº§n tá»­ Ä‘áº§u tiÃªn
            merged_metadata = {**group[0].metadata}
            merged_metadata["chunk_count"] = len(group)

            merged_docs.append(
                Document(page_content=merged_content, metadata=merged_metadata)
            )

        print_document_info(merged_docs, "Káº¿t quáº£ sau khi cluster vÃ  merge")
        return merged_docs
